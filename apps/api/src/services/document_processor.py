# -*- coding: utf-8 -*-
"""
æ–‡æ¡£å¤„ç†æœåŠ¡
æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„è§£æå’Œæ–‡æœ¬æå–
"""
import os
import io
import logging
from typing import Dict, Any, Optional, Tuple
from werkzeug.datastructures import FileStorage

# æ–‡æ¡£è§£æåº“
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import docx2txt
    DOC_AVAILABLE = True
except ImportError:
    DOC_AVAILABLE = False

try:
    import subprocess
    import tempfile
    ANTIWORD_AVAILABLE = True
except ImportError:
    ANTIWORD_AVAILABLE = False

try:
    import openpyxl
    from openpyxl import load_workbook
    XLSX_AVAILABLE = True
except ImportError:
    XLSX_AVAILABLE = False

try:
    import xlrd
    XLS_AVAILABLE = True
except ImportError:
    XLS_AVAILABLE = False

import re
from collections import Counter

try:
    from pptx import Presentation
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False

try:
    from striprtf.striprtf import rtf_to_text
    RTF_AVAILABLE = True
except ImportError:
    RTF_AVAILABLE = False

try:
    import pytesseract
    from PIL import Image
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False

logger = logging.getLogger(__name__)

class DocumentContentCleaner:
    """æ–‡æ¡£å†…å®¹æ¸…æ´—å™¨ - ä¸“é—¨å¤„ç†OCRå™ªå£°å’Œæ ¼å¼åŒ–æ ‡è®°"""
    
    def __init__(self):
        # Wordæ–‡æ¡£ç‰¹æœ‰çš„æ ¼å¼åŒ–æ ‡è®°æ¨¡å¼
        self.word_noise_patterns = [
            r'HYPERLINK\s+"[^"]*"',  # HYPERLINK "url" æ ¼å¼
            r'HYPERLINK\s+[^\s]+',   # HYPERLINK url æ ¼å¼
            r'\bHYPERLINK\b.*?(?=\s|$)',  # ä»»ä½•åŒ…å«HYPERLINKçš„è¡Œ
            r'\bEMBED\b.*?(?=\s|$)',      # EMBED æ ‡è®°
            r'\bMERGEFORMAT\b',           # MERGEFORMAT æ ‡è®°  
            r'\bCERTIFICATE\b',           # CERTIFICATE æ ‡è®°
            r'\bPACKING\b',               # PACKING æ ‡è®°
            r'\b_GoBack\b',               # Wordä¹¦ç­¾æ ‡è®°
            r'\b_Toc\d+\b',               # ç›®å½•æ ‡è®°
        ]
        
        # é¡µé¢å’Œå¯¼èˆªå…ƒç´ æ¨¡å¼
        self.page_nav_patterns = [
            r'PAGE\s+\d+',                # PAGE 7 æ ¼å¼
            r'ç¬¬\s*\d+\s*é¡µ',             # ç¬¬7é¡µ æ ¼å¼
            r'é¡µ\s*ç \s*[:ï¼š]\s*\d+',      # é¡µç ï¼š7 æ ¼å¼
            r'CHAPTER\s+\d+',             # CHAPTER æ ‡è®°
            r'CONTENTS?(?:\s|$)',         # CONTENTS ç›®å½•æ ‡è®°
            r'INDEX(?:\s|$)',             # INDEX ç´¢å¼•æ ‡è®°
            r'TITLE(?:\s|$)',             # TITLE æ ‡è®°
        ]
        
        # è¡¨æ ¼è¾¹æ¡†å’Œæ ¼å¼åŒ–ç¬¦å·
        self.table_noise_patterns = [
            r'^[\|\s\-\+\=]{3,}$',        # è¡¨æ ¼è¾¹æ¡†çº¿
            r'^[A-Za-z]\s+[A-Za-z]\s+[A-Za-z].*[A-Za-z]\s+[A-Za-z]$',  # "A A AB X B" ç±»å‹
            r'^[A-Za-z]{1,2}\s+[a-z]\s+[a-z].*[a-z]\s+[a-z]$',        # "Ca a a a b" ç±»å‹
            r'^[a-z]+\s+\d+\s+[A-Z]+.*$',  # "h 9 HYPERLINK" ç±»å‹
            r'^\s*[ï½œ\|]\s*$',            # å•ç‹¬çš„ç®¡é“ç¬¦
            r'^\s*[\-]{2,}\s*$',          # è¿å­—ç¬¦åˆ†éš”çº¿
        ]
        
        # æ— æ„ä¹‰å­—ç¬¦é‡å¤æ¨¡å¼
        self.meaningless_patterns = [
            r'^(.)\1{5,}$',               # åŒä¸€å­—ç¬¦é‡å¤5æ¬¡ä»¥ä¸Š
            r'^\s*[\.]{3,}\s*$',          # ç‚¹å·çœç•¥ç¬¦
            r'^\s*[\*]{3,}\s*$',          # æ˜Ÿå·è£…é¥°ç¬¦
            r'^\s*[\-]{3,}\s*$',          # è¿å­—ç¬¦åˆ†éš”
            r'^\s*[_]{3,}\s*$',           # ä¸‹åˆ’çº¿åˆ†éš”
        ]
        
        # æŠ€æœ¯è§„æ ¼ä¿æŠ¤æ¨¡å¼ - è¿™äº›æ¨¡å¼çš„å†…å®¹éœ€è¦ä¿ç•™
        self.protect_patterns = [
            r'\d+[VvAaWwHhâ„ƒâ„‰%]',         # åŒ…å«æŠ€æœ¯å•ä½
            r'\d+\s*[-~Â±]\s*\d+',        # æ•°å€¼èŒƒå›´
            r'\d+\s*[:/]\s*\d+',         # æ¯”å€¼æ ¼å¼
            r'(?:ç”µ|å‹|æµ|åŠŸ|ç‡|é¢‘|æ¸©|åº¦|ç²¾|é‡)', # ç”µåŠ›æŠ€æœ¯å…³é”®è¯
            r'\b(?:volt|amp|watt|freq|temp|test|spec)\b', # è‹±æ–‡æŠ€æœ¯è¯ï¼ˆå•è¯è¾¹ç•Œï¼‰
        ]
    
    def clean_document_content(self, content: str) -> Dict[str, Any]:
        """
        å…¨é¢æ¸…æ´—æ–‡æ¡£å†…å®¹
        
        Args:
            content: åŸå§‹æ–‡æ¡£å†…å®¹
            
        Returns:
            Dict: åŒ…å«æ¸…æ´—ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        original_lines = content.split('\n')
        cleaned_lines = []
        noise_stats = {
            'word_noise': 0,
            'page_nav': 0, 
            'table_noise': 0,
            'meaningless': 0,
            'total_removed': 0,
            'total_lines': len(original_lines)
        }
        
        for line in original_lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦ä¿æŠ¤çš„æŠ€æœ¯å†…å®¹
            if self._is_protected_content(line):
                cleaned_lines.append(line)
                continue
            
            # æ£€æŸ¥å„ç§å™ªå£°æ¨¡å¼
            is_noise = False
            
            # Wordæ ¼å¼åŒ–å™ªå£°
            if self._matches_patterns(line, self.word_noise_patterns):
                noise_stats['word_noise'] += 1
                is_noise = True
            
            # é¡µé¢å¯¼èˆªå…ƒç´ 
            elif self._matches_patterns(line, self.page_nav_patterns):
                noise_stats['page_nav'] += 1
                is_noise = True
            
            # è¡¨æ ¼æ ¼å¼åŒ–å™ªå£°
            elif self._matches_patterns(line, self.table_noise_patterns):
                noise_stats['table_noise'] += 1
                is_noise = True
            
            # æ— æ„ä¹‰å­—ç¬¦é‡å¤
            elif self._matches_patterns(line, self.meaningless_patterns):
                noise_stats['meaningless'] += 1
                is_noise = True
            
            if not is_noise:
                # è¿›ä¸€æ­¥æ¸…ç†æœ‰æ•ˆè¡Œä¸­çš„å™ªå£°æ ‡è®°
                cleaned_line = self._clean_line_noise(line)
                if cleaned_line and len(cleaned_line.strip()) > 1:
                    cleaned_lines.append(cleaned_line)
        
        noise_stats['total_removed'] = len(original_lines) - len(cleaned_lines)
        cleaned_content = '\n'.join(cleaned_lines)
        
        logger.info(f"æ–‡æ¡£æ¸…æ´—å®Œæˆ: åŸå§‹{noise_stats['total_lines']}è¡Œ -> æ¸…æ´—å{len(cleaned_lines)}è¡Œ, "
                   f"ç§»é™¤å™ªå£°{noise_stats['total_removed']}è¡Œ")
        
        return {
            'cleaned_content': cleaned_content,
            'original_content': content,
            'noise_statistics': noise_stats,
            'cleaning_ratio': noise_stats['total_removed'] / noise_stats['total_lines'] if noise_stats['total_lines'] > 0 else 0
        }
    
    def _is_protected_content(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦ä¿æŠ¤çš„æŠ€æœ¯å†…å®¹"""
        return any(re.search(pattern, line, re.IGNORECASE) for pattern in self.protect_patterns)
    
    def _matches_patterns(self, text: str, patterns: list) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…ä»»ä½•å™ªå£°æ¨¡å¼"""
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in patterns)
    
    def _clean_line_noise(self, line: str) -> str:
        """æ¸…ç†è¡Œå†…çš„å™ªå£°æ ‡è®°"""
        cleaned = line
        
        # ç§»é™¤Wordæ ¼å¼åŒ–æ ‡è®°
        for pattern in self.word_noise_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    # æ”¯æŒçš„æ–‡æ¡£ç±»å‹
    SUPPORTED_TYPES = {
        # æ–‡æœ¬æ–‡ä»¶
        'text/plain': 'txt',
        
        # PDFæ–‡ä»¶
        'application/pdf': 'pdf',
        
        # Wordæ–‡æ¡£
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',
        'application/msword': 'doc',
        
        # Excelæ–‡ä»¶
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',
        'application/vnd.ms-excel': 'xls',
        
        # PowerPointæ–‡ä»¶
        'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'pptx',
        'application/vnd.ms-powerpoint': 'ppt',
        
        # RTFæ–‡ä»¶
        'application/rtf': 'rtf',
        'text/rtf': 'rtf',
        
        # å›¾ç‰‡æ–‡ä»¶
        'image/png': 'png',
        'image/jpeg': 'jpg',
        'image/jpg': 'jpg',
        'image/gif': 'gif',
        'image/bmp': 'bmp',
        'image/tiff': 'tiff',
    }
    
    def __init__(self):
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        self.max_text_length = 100000  # å¢åŠ åˆ°100kå­—ç¬¦é™åˆ¶ï¼Œæ”¯æŒæ›´é•¿çš„æŠ€æœ¯æ–‡æ¡£
        self.min_text_length = 10  # æœ€å°æ–‡æœ¬é•¿åº¦è¦æ±‚
        self.content_cleaner = DocumentContentCleaner()  # æ–‡æ¡£å†…å®¹æ¸…æ´—å™¨
    
    def is_supported_format(self, mimetype: str, filename: str = "") -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ"""
        if mimetype in self.SUPPORTED_TYPES:
            return True
        
        # åŸºäºæ–‡ä»¶æ‰©å±•åæ£€æŸ¥
        if filename:
            ext = filename.lower().split('.')[-1]
            return ext in ['txt', 'pdf', 'docx', 'doc', 'xlsx', 'xls', 'pptx', 'ppt', 'rtf', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff']
        
        return False
    
    def get_supported_formats(self) -> Dict[str, list]:
        """è·å–æ”¯æŒçš„æ ¼å¼ä¿¡æ¯"""
        return {
            'mimetypes': list(self.SUPPORTED_TYPES.keys()),
            'extensions': ['txt', 'pdf', 'docx', 'doc', 'xlsx', 'xls', 'pptx', 'ppt', 'rtf', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff'],
            'availability': {
                'pdf': PDF_AVAILABLE,
                'docx': DOCX_AVAILABLE,
                'doc': DOC_AVAILABLE,
                'xlsx': XLSX_AVAILABLE,
                'xls': XLS_AVAILABLE,
                'pptx': PPTX_AVAILABLE,
                'rtf': RTF_AVAILABLE,
                'ocr': OCR_AVAILABLE
            }
        }
    
    def process_document(self, file: FileStorage) -> Tuple[str, Dict[str, Any]]:
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            
        Returns:
            Tuple[str, Dict]: (æå–çš„æ–‡æœ¬å†…å®¹, æ–‡æ¡£ä¿¡æ¯)
        """
        # éªŒè¯æ–‡ä»¶
        validation_result = self._validate_file(file)
        if not validation_result['valid']:
            raise ValueError(validation_result['error'])
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        doc_info = {
            'filename': file.filename,
            'mimetype': file.mimetype,
            'size': self._get_file_size(file),
            'type': self._detect_file_type(file.mimetype, file.filename)
        }
        
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
            raw_text_content = self._extract_text(file, doc_info['type'])
            
            # ğŸ”§ æ–‡æ¡£å†…å®¹æ¸…æ´— - ç§»é™¤OCRå™ªå£°å’Œæ ¼å¼åŒ–æ ‡è®°
            cleaning_result = self.content_cleaner.clean_document_content(raw_text_content)
            text_content = cleaning_result['cleaned_content']
            
            # è®°å½•æ¸…æ´—ç»Ÿè®¡ä¿¡æ¯
            doc_info['cleaning_stats'] = cleaning_result['noise_statistics']
            doc_info['cleaning_ratio'] = cleaning_result['cleaning_ratio']
            
            # æ™ºèƒ½æ–‡æœ¬é•¿åº¦å¤„ç†
            if len(text_content) > self.max_text_length:
                # ä½¿ç”¨æ™ºèƒ½æˆªæ–­ï¼Œä¿ç•™é‡è¦ä¿¡æ¯
                text_content = self._intelligent_truncate(text_content)
                doc_info['truncated'] = True
                logger.warning(f"Text content intelligently truncated to {len(text_content)} characters")
            
            doc_info['text_length'] = len(text_content)
            doc_info['word_count'] = len(text_content.split())
            doc_info['original_text_length'] = len(raw_text_content)
            
            # æ—¥å¿—è®°å½•æ¸…æ´—æ•ˆæœ
            if cleaning_result['cleaning_ratio'] > 0.1:  # æ¸…æ´—æ¯”ä¾‹è¶…è¿‡10%æ—¶è®°å½•
                logger.info(f"æ–‡æ¡£æ¸…æ´—æ•ˆæœæ˜¾è‘—: æ¸…æ´—æ¯”ä¾‹{cleaning_result['cleaning_ratio']:.1%}, "
                          f"ç§»é™¤{cleaning_result['noise_statistics']['total_removed']}è¡Œå™ªå£°")
            
            return text_content, doc_info
            
        except Exception as e:
            logger.error(f"Error processing document {file.filename}: {str(e)}")
            raise ValueError(f"Document processing failed: {str(e)}")
    
    def _validate_file(self, file: FileStorage) -> Dict[str, Any]:
        """éªŒè¯ä¸Šä¼ æ–‡ä»¶"""
        if not file:
            return {'valid': False, 'error': 'No file provided'}
        
        if not file.filename:
            return {'valid': False, 'error': 'No filename provided'}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = self._get_file_size(file)
        if file_size > self.max_file_size:
            return {
                'valid': False, 
                'error': f'File size ({file_size / 1024 / 1024:.1f}MB) exceeds limit ({self.max_file_size / 1024 / 1024}MB)'
            }
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if not self.is_supported_format(file.mimetype or '', file.filename):
            return {
                'valid': False,
                'error': f'Unsupported file format: {file.mimetype or "unknown"}'
            }
        
        return {'valid': True}
    
    def _get_file_size(self, file: FileStorage) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        current_pos = file.tell()
        file.seek(0, os.SEEK_END)
        size = file.tell()
        file.seek(current_pos)
        return size
    
    def _detect_file_type(self, mimetype: str, filename: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        if mimetype in self.SUPPORTED_TYPES:
            return self.SUPPORTED_TYPES[mimetype]
        
        # åŸºäºæ‰©å±•åæ£€æµ‹
        if filename:
            ext = filename.lower().split('.')[-1]
            if ext in ['txt']:
                return 'txt'
            elif ext in ['pdf']:
                return 'pdf'
            elif ext in ['docx']:
                return 'docx'
            elif ext in ['doc']:
                return 'doc'
            elif ext in ['xlsx']:
                return 'xlsx'
            elif ext in ['xls']:
                return 'xls'
            elif ext in ['pptx']:
                return 'pptx'
            elif ext in ['ppt']:
                return 'ppt'
            elif ext in ['rtf']:
                return 'rtf'
            elif ext in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff']:
                return ext
        
        return 'unknown'
    
    def _extract_text(self, file: FileStorage, file_type: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬å†…å®¹"""
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        
        if file_type == 'txt':
            return self._extract_text_from_txt(file)
        elif file_type == 'pdf':
            return self._extract_text_from_pdf(file)
        elif file_type == 'docx':
            return self._extract_text_from_docx(file)
        elif file_type == 'doc':
            return self._extract_text_from_doc(file)
        elif file_type == 'xlsx':
            return self._extract_text_from_xlsx(file)
        elif file_type == 'xls':
            return self._extract_text_from_xls(file)
        elif file_type in ['pptx', 'ppt']:
            return self._extract_text_from_pptx(file)
        elif file_type == 'rtf':
            return self._extract_text_from_rtf(file)
        elif file_type in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff']:
            return self._extract_text_from_image(file)
        else:
            raise ValueError(f"Unsupported file type for text extraction: {file_type}")
    
    def _extract_text_from_txt(self, file: FileStorage) -> str:
        """ä»TXTæ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            content = file.read()
            # å°è¯•ä¸åŒç¼–ç 
            for encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†
            return content.decode('utf-8', errors='ignore')
        except Exception as e:
            raise ValueError(f"Failed to read text file: {str(e)}")
    
    def _extract_text_from_pdf(self, file: FileStorage) -> str:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
        if not PDF_AVAILABLE:
            raise ValueError("PDF processing not available. Please install PyPDF2.")
        
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text_content = ""
            
            for page in pdf_reader.pages:
                text_content += page.extract_text() + "\n"
            
            if not text_content.strip():
                raise ValueError("No text content found in PDF. The PDF might be image-based.")
            
            return text_content
            
        except Exception as e:
            raise ValueError(f"Failed to extract text from PDF: {str(e)}")
    
    def _extract_text_from_docx(self, file: FileStorage) -> str:
        """ä»DOCXæ–‡ä»¶æå–æ–‡æœ¬"""
        if not DOCX_AVAILABLE:
            raise ValueError("DOCX processing not available. Please install python-docx.")
        
        try:
            doc = Document(io.BytesIO(file.read()))
            text_content = ""
            
            for paragraph in doc.paragraphs:
                text_content += paragraph.text + "\n"
            
            # æå–è¡¨æ ¼å†…å®¹
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        text_content += cell.text + "\t"
                    text_content += "\n"
            
            return text_content
            
        except Exception as e:
            raise ValueError(f"Failed to extract text from DOCX: {str(e)}")
    
    def _extract_text_from_image(self, file: FileStorage) -> str:
        """ä»å›¾ç‰‡æ–‡ä»¶æå–æ–‡æœ¬(OCR)"""
        if not OCR_AVAILABLE:
            raise ValueError("OCR processing not available. Please install pytesseract and Pillow.")
        
        try:
            image = Image.open(io.BytesIO(file.read()))
            
            # ğŸ”§ å¢å¼ºOCRå¤„ç† - å°è¯•å¤šç§OCRé…ç½®
            ocr_configs = [
                '--psm 6',    # ç»Ÿä¸€æ–‡æœ¬å—
                '--psm 4',    # å‡è®¾ä¸ºå•åˆ—æ–‡æœ¬
                '--psm 3',    # å…¨è‡ªåŠ¨é¡µé¢åˆ†å‰²ä½†ä¸è¿›è¡Œæ–¹å‘å’Œè„šæœ¬æ£€æµ‹
                '--psm 8',    # å°†å›¾åƒè§†ä¸ºå•ä¸ªå•è¯
                '--psm 13'    # åŸå§‹è¡Œã€‚å°†å›¾åƒè§†ä¸ºå•ä¸ªæ–‡æœ¬è¡Œ
            ]
            
            best_text = ""
            best_score = 0
            
            for config in ocr_configs:
                try:
                    # ä½¿ç”¨tesseractè¿›è¡ŒOCRï¼Œæ”¯æŒä¸­è‹±æ–‡
                    text_content = pytesseract.image_to_string(
                        image, 
                        lang='chi_sim+eng',  # ä¸­æ–‡ç®€ä½“+è‹±æ–‡
                        config=config
                    )
                    
                    if text_content and text_content.strip():
                        # è¯„ä¼°OCRç»“æœè´¨é‡
                        quality_score = self._evaluate_ocr_quality(text_content)
                        logger.info(f"OCRé…ç½® {config} è´¨é‡è¯„åˆ†: {quality_score}")
                        
                        if quality_score > best_score:
                            best_text = text_content
                            best_score = quality_score
                            
                        # å¦‚æœè´¨é‡å¾ˆé«˜ï¼Œæå‰é€€å‡º
                        if quality_score > 0.8:
                            break
                            
                except Exception as ocr_error:
                    logger.debug(f"OCRé…ç½® {config} å¤±è´¥: {str(ocr_error)}")
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬ï¼Œå°è¯•è‹±æ–‡ä¸“ç”¨OCR
            if not best_text.strip():
                try:
                    text_content = pytesseract.image_to_string(
                        image,
                        lang='eng',  # ä»…è‹±æ–‡
                        config='--psm 6'
                    )
                    if text_content and text_content.strip():
                        quality_score = self._evaluate_ocr_quality(text_content)
                        if quality_score > best_score:
                            best_text = text_content
                            best_score = quality_score
                            logger.info(f"è‹±æ–‡OCRè´¨é‡è¯„åˆ†: {quality_score}")
                except Exception as e:
                    logger.debug(f"è‹±æ–‡OCRå¤±è´¥: {str(e)}")
            
            # æ¸…ç†OCRç»“æœ
            if best_text.strip():
                cleaned_text = self._clean_ocr_text(best_text)
                if len(cleaned_text.strip()) >= 10:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                    logger.info(f"OCRæˆåŠŸæå–æ–‡æœ¬ï¼Œè´¨é‡è¯„åˆ†: {best_score:.2f}")
                    return cleaned_text
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
            raise ValueError(
                "æ— æ³•ä»å›¾ç‰‡ä¸­æå–å¯è¯»æ–‡æœ¬ã€‚å¯èƒ½çš„åŸå› ï¼š\n"
                "1. å›¾ç‰‡ä¸­æ²¡æœ‰æ–‡æœ¬å†…å®¹\n"
                "2. å›¾ç‰‡åˆ†è¾¨ç‡è¿‡ä½æˆ–è´¨é‡è¾ƒå·®\n"
                "3. æ–‡å­—é¢œè‰²ä¸èƒŒæ™¯å¯¹æ¯”åº¦ä¸è¶³\n"
                "4. æ–‡å­—è§’åº¦å€¾æ–œæˆ–æ–¹å‘ä¸æ­£ç¡®\n\n"
                "å»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n"
                "â€¢ æä¾›æ›´é«˜åˆ†è¾¨ç‡çš„å›¾ç‰‡\n"
                "â€¢ ç¡®ä¿æ–‡å­—æ¸…æ™°å¯è¯»\n"
                "â€¢ å°†å›¾ç‰‡è½¬æ¢ä¸ºæ–‡æ¡£æ ¼å¼ï¼ˆPDFã€Wordç­‰ï¼‰"
            )
            
        except Exception as e:
            if "æ— æ³•ä»å›¾ç‰‡ä¸­æå–å¯è¯»æ–‡æœ¬" in str(e):
                raise e
            else:
                raise ValueError(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}")
    
    def _extract_text_from_doc(self, file: FileStorage) -> str:
        """ä»DOCæ–‡ä»¶æå–æ–‡æœ¬ - ä½¿ç”¨å¤šç§æ–¹æ³•å°è¯•"""
        file_content = file.read()
        best_text = ""
        best_score = 0
        extraction_methods = []
        
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨native antiwordå·¥å…· (å¦‚æœå­˜åœ¨)
        try:
            with tempfile.NamedTemporaryFile(suffix='.doc', delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file.flush()
                
                try:
                    # å°è¯•ç³»ç»Ÿçº§antiwordå‘½ä»¤
                    result = subprocess.run(
                        ['/usr/local/bin/antiword', temp_file.name],
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    
                    if result.returncode == 0 and result.stdout.strip():
                        quality_score = self._evaluate_text_quality(result.stdout)
                        extraction_methods.append(("native-antiword", result.stdout, quality_score))
                        logger.info(f"Native antiword extraction quality score: {quality_score}")
                        if quality_score > best_score:
                            best_text = result.stdout
                            best_score = quality_score
                    else:
                        logger.debug(f"Native antiword failed with return code {result.returncode}")
                        
                except (subprocess.TimeoutExpired, FileNotFoundError) as e:
                    logger.debug(f"Native antiword command not found: {str(e)}")
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(temp_file.name)
                    except:
                        pass
                        
        except Exception as e:
            logger.debug(f"Native antiword method failed: {str(e)}")
        
        # æ–¹æ³•2: å°è¯•ä½¿ç”¨docx2txt (é€‚ç”¨äºä¸€äº›.docæ–‡ä»¶)
        if DOC_AVAILABLE:
            try:
                file.seek(0)
                text_content = docx2txt.process(io.BytesIO(file_content))
                if text_content and text_content.strip():
                    quality_score = self._evaluate_text_quality(text_content)
                    extraction_methods.append(("docx2txt", text_content, quality_score))
                    logger.info(f"docx2txt extraction quality score: {quality_score}")
                    if quality_score > best_score:
                        best_text = text_content
                        best_score = quality_score
            except Exception as e:
                logger.debug(f"docx2txt failed: {str(e)}")
        
        # æ–¹æ³•3: OLEæ–‡ä»¶ç»“æ„è§£æ (æ–°å¢æ–¹æ³•)
        try:
            import olefile
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„OLEæ–‡ä»¶
            if olefile.isOleFile(io.BytesIO(file_content)):
                with olefile.OleFileIO(io.BytesIO(file_content)) as ole:
                    # å°è¯•è¯»å–WordDocumentæµ
                    try:
                        # æ£€æŸ¥WordDocumentæµæ˜¯å¦å­˜åœ¨
                        ole.openstream('WordDocument')
                        ole_has_word_stream = True
                    except:
                        ole_has_word_stream = False
                    
                    if ole_has_word_stream:
                        try:
                            word_stream = ole.openstream('WordDocument')
                            raw_data = word_stream.read()
                            
                            # å°è¯•å¤šç§ç¼–ç è§£æ
                            encodings_to_try = ['utf-16le', 'utf-16', 'gbk', 'gb2312', 'utf-8', 'latin1']
                            
                            for encoding in encodings_to_try:
                                try:
                                    decoded_text = raw_data.decode(encoding, errors='ignore')
                                    clean_text = self._clean_ole_text(decoded_text)
                                    
                                    if len(clean_text.strip()) > 100:  # è‡³å°‘éœ€è¦100ä¸ªå­—ç¬¦
                                        quality_score = self._evaluate_text_quality(clean_text)
                                        extraction_methods.append((f"ole-{encoding}", clean_text, quality_score))
                                        logger.info(f"OLE {encoding} extraction quality score: {quality_score}")
                                        if quality_score > best_score:
                                            best_text = clean_text
                                            best_score = quality_score
                                except:
                                    continue
                                    
                        except Exception as e:
                            logger.debug(f"WordDocument stream extraction failed: {e}")
                            
        except Exception as e:
            logger.debug(f"OLE extraction method failed: {str(e)}")
        
        # æ–¹æ³•4: å¢å¼ºç¼–ç æ£€æµ‹å’Œè½¬æ¢
        # ğŸ”§ ä½¿ç”¨æ›´å¤šç¼–ç æ–¹å¼å’Œæ£€æµ‹ç­–ç•¥
        encodings_to_try = [
            # ä¸­æ–‡ç¼–ç  - ä¼˜å…ˆå°è¯•
            'gb18030', 'gbk', 'gb2312', 'big5', 'hz',
            # Unicodeç¼–ç 
            'utf-8', 'utf-16', 'utf-16le', 'utf-16be', 'utf-32',
            # Windowsç¼–ç 
            'cp936', 'cp950', 'cp1252', 'cp1251',
            # ISOç¼–ç 
            'iso-8859-1', 'latin1'
        ]
        
        # ğŸ”§ å¢å¼ºç¼–ç æ£€æµ‹ - å…ˆå°è¯•æ£€æµ‹æœ€å¯èƒ½çš„ç¼–ç 
        try:
            import chardet
            
            # æ£€æµ‹æ–‡ä»¶ç¼–ç 
            file.seek(0)
            sample = file.read(min(10240, len(file_content)))  # è¯»å–å‰10KBæ£€æµ‹ç¼–ç 
            encoding_result = chardet.detect(sample)
            
            if encoding_result and encoding_result.get('encoding') and encoding_result.get('confidence', 0) > 0.7:
                detected_encoding = encoding_result['encoding'].lower()
                logger.info(f"æ£€æµ‹åˆ°ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {encoding_result['confidence']})")
                
                # å°†æ£€æµ‹åˆ°çš„ç¼–ç æ”¾åœ¨åˆ—è¡¨é¦–ä½
                if detected_encoding not in [enc.lower() for enc in encodings_to_try]:
                    encodings_to_try.insert(0, detected_encoding)
                else:
                    # å°†æ£€æµ‹åˆ°çš„ç¼–ç ç§»åˆ°å‰é¢
                    for i, enc in enumerate(encodings_to_try):
                        if enc.lower() == detected_encoding:
                            encodings_to_try.insert(0, encodings_to_try.pop(i))
                            break
                            
        except ImportError:
            logger.debug("chardetæ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡è‡ªåŠ¨ç¼–ç æ£€æµ‹")
        except Exception as e:
            logger.debug(f"ç¼–ç æ£€æµ‹å¤±è´¥: {str(e)}")
        
        # å°è¯•ä¸åŒç¼–ç 
        for encoding in encodings_to_try:
            try:
                text_content = file_content.decode(encoding, errors='replace')  # ä½¿ç”¨replaceè€Œä¸æ˜¯ignore
                
                # ğŸ”§ æ›´ä¸¥æ ¼çš„æ–‡æœ¬éªŒè¯
                if self._is_corrupted_text(text_content):
                    logger.debug(f"ç¼–ç  {encoding} äº§ç”Ÿä¹±ç ï¼Œè·³è¿‡")
                    continue
                
                # é«˜çº§æ–‡æœ¬æ¸…ç†
                clean_text = self._clean_extracted_text(text_content)
                if len(clean_text.strip()) > self.min_text_length:
                    quality_score = self._evaluate_text_quality(clean_text)
                    extraction_methods.append((f"encoding-{encoding}", clean_text, quality_score))
                    logger.debug(f"Encoding {encoding} extraction quality score: {quality_score}")
                    if quality_score > best_score:
                        best_text = clean_text
                        best_score = quality_score
                        
                    # å¦‚æœè´¨é‡åˆ†æ•°å¾ˆé«˜ï¼Œæå‰é€€å‡º
                    if quality_score > 0.8:
                        logger.info(f"æ‰¾åˆ°é«˜è´¨é‡æ–‡æœ¬ (è´¨é‡åˆ†æ•°: {quality_score})ï¼Œä½¿ç”¨ç¼–ç : {encoding}")
                        break
                        
            except Exception as e:
                logger.debug(f"ç¼–ç  {encoding} è§£ç å¤±è´¥: {str(e)}")
                continue
        
        # è®°å½•æ‰€æœ‰å°è¯•çš„æ–¹æ³•
        logger.info(f"DOC extraction attempted {len(extraction_methods)} methods, best score: {best_score}")
        
        # å¦‚æœæ‰¾åˆ°äº†å¯ç”¨çš„æ–‡æœ¬
        if best_text and best_score > 0.1:  # æœ€ä½è´¨é‡é˜ˆå€¼
            logger.info(f"Successfully extracted DOC text with quality score {best_score}")
            return best_text
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
        error_msg = (
            "æ— æ³•ä»æ­¤.docæ–‡ä»¶ä¸­æå–å¯è¯»æ–‡æœ¬ã€‚å¯èƒ½çš„åŸå› ï¼š"
            "\n1. æ–‡ä»¶æŸåæˆ–æ ¼å¼ä¸æ ‡å‡†"
            "\n2. æ–‡ä»¶åŒ…å«ä¸»è¦æ˜¯å›¾ç‰‡æˆ–å›¾è¡¨"
            "\n3. æ–‡ä»¶å—å¯†ç ä¿æŠ¤"
            "\n4. æ–‡ä»¶ä½¿ç”¨äº†ç‰¹æ®Šçš„ç¼–ç æ ¼å¼"
            "\n\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š"
            "\nâ€¢ è¯·å°è¯•å°†æ–‡ä»¶è½¬æ¢ä¸º.docxæ ¼å¼"
            "\nâ€¢ ç¡®ä¿æ–‡ä»¶åŒ…å«å¯è¯»çš„æ–‡æœ¬å†…å®¹"
            "\nâ€¢ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸‹è½½"
            f"\nâ€¢ å°è¯•çš„æå–æ–¹æ³•æ•°é‡: {len(extraction_methods)}"
        )
        
        raise ValueError(error_msg)
    
    def _extract_text_from_xlsx(self, file: FileStorage) -> str:
        """ä»XLSXæ–‡ä»¶æå–æ–‡æœ¬"""
        if not XLSX_AVAILABLE:
            raise ValueError("XLSX processing not available. Please install openpyxl.")
        
        try:
            workbook = load_workbook(io.BytesIO(file.read()), read_only=True, data_only=True)
            text_content = ""
            
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                text_content += f"=== Sheet: {sheet_name} ===\n"
                
                for row in sheet.iter_rows(values_only=True):
                    row_text = []
                    for cell in row:
                        if cell is not None:
                            row_text.append(str(cell))
                    if row_text:
                        text_content += "\t".join(row_text) + "\n"
                
                text_content += "\n"
            
            workbook.close()
            
            if not text_content.strip():
                raise ValueError("No text content found in XLSX file.")
            
            return text_content
            
        except Exception as e:
            raise ValueError(f"Failed to extract text from XLSX: {str(e)}")
    
    def _extract_text_from_xls(self, file: FileStorage) -> str:
        """ä»XLSæ–‡ä»¶æå–æ–‡æœ¬"""
        if not XLS_AVAILABLE:
            raise ValueError("XLS processing not available. Please install xlrd.")
        
        try:
            workbook = xlrd.open_workbook(file_contents=file.read())
            text_content = ""
            
            for sheet_idx in range(workbook.nsheets):
                sheet = workbook.sheet_by_index(sheet_idx)
                text_content += f"=== Sheet: {sheet.name} ===\n"
                
                for row_idx in range(sheet.nrows):
                    row_values = []
                    for col_idx in range(sheet.ncols):
                        cell = sheet.cell(row_idx, col_idx)
                        if cell.value:
                            row_values.append(str(cell.value))
                    if row_values:
                        text_content += "\t".join(row_values) + "\n"
                
                text_content += "\n"
            
            if not text_content.strip():
                raise ValueError("No text content found in XLS file.")
            
            return text_content
            
        except Exception as e:
            raise ValueError(f"Failed to extract text from XLS: {str(e)}")
    
    def _extract_text_from_pptx(self, file: FileStorage) -> str:
        """ä»PPTX/PPTæ–‡ä»¶æå–æ–‡æœ¬"""
        if not PPTX_AVAILABLE:
            raise ValueError("PPTX processing not available. Please install python-pptx.")
        
        try:
            presentation = Presentation(io.BytesIO(file.read()))
            text_content = ""
            
            for slide_idx, slide in enumerate(presentation.slides):
                text_content += f"=== Slide {slide_idx + 1} ===\n"
                
                # æå–å¹»ç¯ç‰‡ä¸­çš„æ‰€æœ‰æ–‡æœ¬
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        text_content += shape.text + "\n"
                    elif hasattr(shape, "text_frame") and shape.text_frame:
                        for paragraph in shape.text_frame.paragraphs:
                            for run in paragraph.runs:
                                text_content += run.text
                            text_content += "\n"
                
                text_content += "\n"
            
            if not text_content.strip():
                raise ValueError("No text content found in PPTX file.")
            
            return text_content
            
        except Exception as e:
            raise ValueError(f"Failed to extract text from PPTX: {str(e)}")
    
    def _extract_text_from_rtf(self, file: FileStorage) -> str:
        """ä»RTFæ–‡ä»¶æå–æ–‡æœ¬"""
        if not RTF_AVAILABLE:
            raise ValueError("RTF processing not available. Please install striprtf.")
        
        try:
            rtf_content = file.read().decode('utf-8', errors='ignore')
            text_content = rtf_to_text(rtf_content)
            
            if not text_content or not text_content.strip():
                raise ValueError("No text content found in RTF file.")
            
            return text_content
            
        except Exception as e:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                file.seek(0)
                rtf_content = file.read().decode('latin1', errors='ignore')
                text_content = rtf_to_text(rtf_content)
                
                if text_content and text_content.strip():
                    return text_content
                else:
                    raise ValueError("No text content found in RTF file.")
                    
            except Exception as e2:
                raise ValueError(f"Failed to extract text from RTF: {str(e)} / {str(e2)}")
    
    def _clean_extracted_text(self, text: str) -> str:
        """æ¸…ç†ä»æ–‡æ¡£ä¸­æå–çš„æ–‡æœ¬ - å¢å¼ºç‰ˆï¼Œä¸“é—¨å¤„ç†.docæ–‡ä»¶çš„äºŒè¿›åˆ¶åƒåœ¾æ•°æ®"""
        import re
        
        # 1. ğŸ›¡ï¸ é«˜çº§äºŒè¿›åˆ¶åƒåœ¾è¿‡æ»¤ - ç§»é™¤æ˜æ˜¾çš„ç¼–ç é”™è¯¯å­—ç¬¦
        # ç§»é™¤æ§åˆ¶å­—ç¬¦å’Œéæ‰“å°å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œç©ºæ ¼ï¼‰
        cleaned = ''.join(char for char in text if char.isprintable() or char in '\n\t ')
        
        # ğŸ”§ ç§»é™¤.docè§£æå¸¸è§çš„ä¹±ç å­—ç¬¦æ¨¡å¼
        doc_garbage_patterns = [
            r'[æ½—æ‘²æ¥ç‰£ç¯æ™¯ç……æ…µæ¥´æ¹¯ç•±ç‘¡æ½©åæ½—æ‘²æ½„å€ç‘©æ•¬ç‰æ…©è¢ˆéœ¡è ˆè¢¢]+',  # å¸¸è§.docä¹±ç å­—ç¬¦ä¸²
            r'[ã¸³ã ´ã”·ã¤¸ãœ¹ãˆ°ã±ã ²ãŒ³ã˜´ã”µã˜¶ã ·ã¤¸ã ¹]+',  # åå…­è¿›åˆ¶ä¹±ç 
            r'[å±œå±å±¬å±­å±¨å±ªå±¢å±£å±¤å±¥å±¦å±§å±¨å±©å±²]+',  # Wordç»“æ„å­—ç¬¦
            r'[â–‰â–Šâ–‹â–Œâ–â–â–â–ˆâ–„â–€â– â–¡â–²â–³â–¼â–½â—†â—‡â—‹â—â—â˜†â˜…]+',  # OLEå›¾å½¢å­—ç¬¦
            r'[\u0080-\u00ff]{2,}',              # Latin-1æ‰©å±•å­—ç¬¦ä¹±ç 
            r'[\ue000-\uf8ff]+',                 # ç§ç”¨åŒºå­—ç¬¦
            r'[\ufeff\ufffe]+',                  # å­—èŠ‚åºæ ‡è®°
        ]
        
        for pattern in doc_garbage_patterns:
            cleaned = re.sub(pattern, ' ', cleaned)
            
        # ğŸ”§ ç§»é™¤ç¼–ç é”™è¯¯äº§ç”Ÿçš„æ›¿ä»£å­—ç¬¦å’Œé—®å·ä¸²
        cleaned = re.sub(r'[ï¿½?]{2,}', ' ', cleaned)
        
        # 2. ğŸ” æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å’Œè¯†åˆ«
        # ä½¿ç”¨æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ä¸­æ–‡æ–‡æœ¬å—
        chinese_pattern = r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]+'
        chinese_matches = re.findall(chinese_pattern, cleaned)
        
        # æŸ¥æ‰¾è‹±æ–‡æ–‡æœ¬å—ï¼ˆæ›´ä¸¥æ ¼çš„æ¨¡å¼ï¼Œæ’é™¤ä¹±ç ï¼‰
        english_pattern = r'[a-zA-Z][a-zA-Z0-9\s\.,;:!?\-()\'"\[\]{}]{3,}'
        english_matches = re.findall(english_pattern, cleaned)
        
        # æŸ¥æ‰¾æŠ€æœ¯æ•°æ®å—ï¼ˆæ•°å­—+å•ä½çš„æ¨¡å¼ï¼‰
        tech_data_pattern = r'\d+[\w\s]*[A-Za-z%Â°â„ƒÎ©/\-]{1,5}'
        tech_matches = re.findall(tech_data_pattern, cleaned)
        
        # 3. ğŸ§¹ æ™ºèƒ½æ–‡æœ¬é‡ç»„
        meaningful_text = []
        
        # æ·»åŠ ä¸­æ–‡æ–‡æœ¬ï¼ˆæ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼‰
        for match in chinese_matches:
            clean_match = match.strip()
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ±‰å­—å†…å®¹
            chinese_char_count = len(re.findall(r'[\u4e00-\u9fff]', clean_match))
            if chinese_char_count >= 2 and len(clean_match) >= 3:
                # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯ä¹±ç æ±‰å­—
                if not self._contains_garbage_chinese(clean_match):
                    meaningful_text.append(clean_match)
        
        # æ·»åŠ è‹±æ–‡æ–‡æœ¬ï¼ˆè¿‡æ»¤æ‰å¯èƒ½çš„ä¹±ç ï¼‰
        for match in english_matches:
            clean_match = match.strip()
            words = clean_match.split()
            # æ›´ä¸¥æ ¼çš„è‹±æ–‡å†…å®¹éªŒè¯
            if (len(words) >= 2 and len(clean_match) >= 8 and 
                self._is_meaningful_english(clean_match)):
                meaningful_text.append(clean_match)
        
        # æ·»åŠ æŠ€æœ¯æ•°æ®
        for match in tech_matches:
            clean_match = match.strip()
            if len(clean_match) >= 3 and not self._is_corrupted_tech_data(clean_match):
                meaningful_text.append(clean_match)
        
        # 4. ğŸ”„ å¦‚æœæ™ºèƒ½æå–å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆçš„åŸºæœ¬æ¸…ç†
        if not meaningful_text:
            logger.warning("æ™ºèƒ½æ–‡æœ¬æå–æœªæ‰¾åˆ°æœ‰æ„ä¹‰å†…å®¹ï¼Œä½¿ç”¨ä¿å®ˆæ¸…ç†")
            # åŸºæœ¬æ¸…ç†ï¼šç§»é™¤è¿ç»­çš„ç©ºç™½å­—ç¬¦
            cleaned = re.sub(r'\s+', ' ', cleaned)
            # ç§»é™¤æ˜æ˜¾çš„åƒåœ¾è¡Œ
            lines = []
            for line in cleaned.split('\n'):
                line = line.strip()
                if (line and len(line) > 2 and 
                    not self._is_obvious_garbage_line(line)):
                    lines.append(line)
            return '\n'.join(lines)
        
        # 5. ğŸ¯ ç»„åˆæœ‰æ„ä¹‰çš„æ–‡æœ¬
        result = '\n'.join(meaningful_text)
        
        # 6. ğŸ§¹ æœ€ç»ˆæ¸…ç†å’Œè§„èŒƒåŒ–
        result = re.sub(r'\n\s*\n', '\n\n', result)  # è§„èŒƒæ®µè½é—´è·
        result = re.sub(r'[ \t]+', ' ', result)      # è§„èŒƒç©ºæ ¼
        result = re.sub(r'\n{3,}', '\n\n', result)   # é™åˆ¶æœ€å¤šåŒæ¢è¡Œ
        
        return result.strip()
    
    def _clean_ole_text(self, text: str) -> str:
        """ä¸“é—¨æ¸…ç†ä»OLEæ–‡ä»¶ç»“æ„ä¸­æå–çš„æ–‡æœ¬"""
        import re
        
        # 1. ç§»é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦å’Œéæ‰“å°å­—ç¬¦
        cleaned = ''.join(char for char in text if char.isprintable() or char in '\n\t ')
        
        # 2. ç§»é™¤Wordæ–‡æ¡£ç‰¹æœ‰çš„ç»“æ„åŒ–æ•°æ®
        # ç§»é™¤OLEå¯¹è±¡æ ‡è¯†ç¬¦
        cleaned = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', cleaned)
        
        # 3. æŸ¥æ‰¾å¹¶æå–æœ‰æ„ä¹‰çš„æ–‡æœ¬æ®µè½
        # å¯»æ‰¾ä¸­æ–‡å­—ç¬¦é›†ç¾¤
        chinese_pattern = r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]{2,}'
        chinese_blocks = re.findall(chinese_pattern, cleaned)
        
        # å¯»æ‰¾è‹±æ–‡å­—ç¬¦é›†ç¾¤
        english_pattern = r'[a-zA-Z][a-zA-Z0-9\s\.,;:!?\-()\'"\[\]{}]{5,}'
        english_blocks = re.findall(english_pattern, cleaned)
        
        # 4. è¿‡æ»¤å’Œé‡æ–°ç»„ç»‡æ–‡æœ¬
        meaningful_blocks = []
        
        # å¤„ç†ä¸­æ–‡æ–‡æœ¬å—
        for block in chinese_blocks:
            clean_block = block.strip()
            if len(clean_block) >= 3:  # è‡³å°‘3ä¸ªå­—ç¬¦
                meaningful_blocks.append(clean_block)
        
        # å¤„ç†è‹±æ–‡æ–‡æœ¬å—
        for block in english_blocks:
            clean_block = block.strip()
            words = clean_block.split()
            if len(words) >= 3 and len(clean_block) >= 10:  # è‡³å°‘3ä¸ªå•è¯ä¸”10ä¸ªå­—ç¬¦
                meaningful_blocks.append(clean_block)
        
        # 5. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œå›é€€åˆ°åŸºç¡€æ¸…ç†
        if not meaningful_blocks:
            # ç§»é™¤é‡å¤çš„ç©ºç™½å­—ç¬¦
            cleaned = re.sub(r'\s+', ' ', cleaned)
            # ç§»é™¤æ˜æ˜¾çš„ä¹±ç æ¨¡å¼
            cleaned = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\w\s\.,;:!?\-()\'"\[\]{}]', '', cleaned)
            return cleaned.strip()
        
        # 6. ç»„åˆæœ‰æ„ä¹‰çš„æ–‡æœ¬å—
        result = '\n'.join(meaningful_blocks)
        
        # 7. æœ€ç»ˆæ¸…ç†
        result = re.sub(r'\n+', '\n\n', result)  # è§„èŒƒæ®µè½é—´è·
        result = re.sub(r'[ \t]+', ' ', result)  # è§„èŒƒç©ºæ ¼
        
        return result.strip()
    
    def _intelligent_truncate(self, text: str) -> str:
        """æ™ºèƒ½æˆªæ–­æ–‡æœ¬ï¼Œä¿ç•™é‡è¦ä¿¡æ¯"""
        if len(text) <= self.max_text_length:
            return text
        
        # åˆ†å‰²æˆæ®µè½
        paragraphs = text.split('\n\n')
        
        # ä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®ä¿¡æ¯çš„æ®µè½
        important_keywords = [
            # ä¸­æ–‡å…³é”®è¯
            'äº§å“', 'å‹å·', 'è§„æ ¼', 'å‚æ•°', 'æŠ€æœ¯', 'ç”µå‹', 'ç”µæµ', 'åŠŸç‡', 'é¢‘ç‡', 
            'æµ‹è¯•', 'ä¿æŠ¤', 'ç»§ç”µ', 'è£…ç½®', 'è®¾å¤‡', 'æ€§èƒ½', 'ç²¾åº¦', 'èŒƒå›´', 
            'æ¸©åº¦', 'æ¹¿åº¦', 'ç¯å¢ƒ', 'è®¤è¯', 'æ ‡å‡†', 'è´¨é‡', 'ä¿ä¿®', 'æœåŠ¡',
            'é…ç½®', 'æ¥å£', 'é€šä¿¡', 'æ§åˆ¶', 'æ˜¾ç¤º', 'æ“ä½œ', 'å®‰è£…', 'ç»´æŠ¤',
            
            # è‹±æ–‡å…³é”®è¯
            'product', 'model', 'specification', 'parameter', 'technical', 
            'voltage', 'current', 'power', 'frequency', 'test', 'protection',
            'relay', 'device', 'equipment', 'performance', 'accuracy', 'range',
            'temperature', 'humidity', 'environment', 'certification', 'standard',
            'quality', 'warranty', 'service', 'configuration', 'interface',
            'communication', 'control', 'display', 'operation', 'installation'
        ]
        
        # æŒ‰é‡è¦æ€§å¯¹æ®µè½æ’åº
        scored_paragraphs = []
        for para in paragraphs:
            score = 0
            para_lower = para.lower()
            
            # è®¡ç®—å…³é”®è¯å¾—åˆ†
            for keyword in important_keywords:
                score += para_lower.count(keyword.lower()) * 10
            
            # åŒ…å«æ•°å­—å’Œå•ä½çš„æ®µè½æ›´é‡è¦ï¼ˆæŠ€æœ¯è§„æ ¼ï¼‰
            import re
            if re.search(r'\d+[A-Za-z]*[\s]*[A-Za-z/Î©%Â°â„ƒ]', para):
                score += 20
            
            # åŒ…å«è¡¨æ ¼ç»“æ„çš„æ®µè½
            if '\t' in para or para.count('|') > 2:
                score += 15
            
            # æ®µè½é•¿åº¦é€‚ä¸­çš„æ›´é‡è¦
            if 50 <= len(para) <= 500:
                score += 5
            
            scored_paragraphs.append((score, para))
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œä¿ç•™é«˜åˆ†æ®µè½
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        result = []
        current_length = 0
        target_length = int(self.max_text_length * 0.95)  # ç•™ä¸€äº›ä½™é‡
        
        # é¦–å…ˆæ·»åŠ æœ€é‡è¦çš„æ®µè½
        for score, para in scored_paragraphs:
            if current_length + len(para) + 2 <= target_length:  # +2 for \n\n
                result.append(para)
                current_length += len(para) + 2
            elif current_length < target_length * 0.7:  # å¦‚æœè¿˜æ²¡è¾¾åˆ°70%ï¼Œå°è¯•éƒ¨åˆ†æˆªå–
                remaining = target_length - current_length - 2
                if remaining > 100:  # åªæœ‰è¶³å¤Ÿé•¿åº¦æ‰æˆªå–
                    truncated_para = para[:remaining] + "..."
                    result.append(truncated_para)
                    break
            else:
                break
        
        final_text = '\n\n'.join(result)
        
        # å¦‚æœç»“æœå¤ªçŸ­ï¼Œä»åŸæ–‡å¼€å¤´è¡¥å……
        if len(final_text) < target_length * 0.5:
            remaining_space = target_length - len(final_text)
            if remaining_space > 100:
                prefix = text[:remaining_space-len(final_text)-20] + "...\n\n"
                final_text = prefix + final_text
        
        logger.info(f"æ™ºèƒ½æˆªæ–­ï¼šåŸæ–‡{len(text)}å­—ç¬¦ -> {len(final_text)}å­—ç¬¦ï¼Œä¿ç•™{len(result)}ä¸ªæ®µè½")
        return final_text
    
    def _is_corrupted_text(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºä¹±ç """
        if not text or len(text.strip()) < 20:
            return True
        
        # ğŸ” æ£€æµ‹ä¹±ç çš„å‡ ä¸ªæŒ‡æ ‡
        text_sample = text[:2000]  # å–å‰2000å­—ç¬¦ä½œä¸ºæ ·æœ¬
        
        # 1. æ£€æµ‹å¼‚å¸¸é«˜é¢‘çš„ç‰¹æ®Šå­—ç¬¦
        special_chars = sum(1 for c in text_sample if ord(c) > 0xFFFF or c in 'æ€é‡¨å¨é†¢è— ä¿¹ç‰æ…µæ¥´æ¹¯ç•±ç‘¡æ½©')
        if special_chars / len(text_sample) > 0.3:  # è¶…è¿‡30%çš„å¼‚å¸¸å­—ç¬¦
            logger.debug(f"æ£€æµ‹åˆ°é«˜é¢‘å¼‚å¸¸å­—ç¬¦: {special_chars}/{len(text_sample)}")
            return True
        
        # 2. æ£€æµ‹é‡å¤æ¨¡å¼ (å¸¸è§ä¹±ç ç‰¹å¾) - è°ƒæ•´é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦æ£€æµ‹
        import re
        # æ£€æµ‹é‡å¤çš„çŸ­å­—ç¬¦ä¸²æ¨¡å¼ï¼ˆå¦‚"æ©¢æ©¢"ã€"å¡…å¡…"ç­‰ï¼‰
        repeated_patterns = re.findall(r'(.{1,3})\1{5,}', text_sample)
        if len(repeated_patterns) > 15:  # æé«˜é˜ˆå€¼
            logger.debug(f"æ£€æµ‹åˆ°å¤§é‡é‡å¤æ¨¡å¼: {len(repeated_patterns)}")
            return True
        
        # 3. æ£€æµ‹æ˜¯å¦åŒ…å«å¯è¯»çš„ä¸­æ–‡æˆ–è‹±æ–‡å†…å®¹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text_sample))
        english_words = len(re.findall(r'\b[a-zA-Z]{3,}\b', text_sample))
        readable_content_ratio = (chinese_chars + english_words * 5) / len(text_sample)
        
        if readable_content_ratio < 0.05:  # å¯è¯»å†…å®¹å°‘äº5%
            logger.debug(f"æ£€æµ‹åˆ°å¯è¯»å†…å®¹è¿‡å°‘: ä¸­æ–‡å­—ç¬¦{chinese_chars}, è‹±æ–‡å•è¯{english_words}")
            return True
        
        # 4. æ£€æµ‹ç¼–ç é—®é¢˜ç‰¹å¾ - æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„ç¼–ç é”™è¯¯æ¨¡å¼
        encoding_error_patterns = [
            r'[æ½—æ‘²|æ¥ç‰£ç¯æ™¯|ç……æ…µæ¥´æ¹¯|ç•±ç‘¡æ½©|åæ½—æ‘²æ½„|å€ç‘©æ•¬|ç‰æ…©]{5,}',  # è¿ç»­çš„ç¼–ç é”™è¯¯å­—ç¬¦
            r'[è¢ˆè¢ˆè¢ˆ|éœ¡è ˆè¢¢]{3,}',  # é‡å¤çš„ä¹±ç å­—ç¬¦
            r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x84\x86-\x9F]{2,}'  # æ§åˆ¶å­—ç¬¦
        ]
        
        for pattern in encoding_error_patterns:
            if re.search(pattern, text_sample):
                logger.debug(f"æ£€æµ‹åˆ°ç¼–ç é”™è¯¯æ¨¡å¼: {pattern}")
                return True
        
        return False
    
    def _calculate_encoding_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç¼–ç è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼‰"""
        if not text:
            return 0.0
        
        score = 0.0
        
        # 1. æ£€æŸ¥UTF-8ç¼–ç æœ‰æ•ˆæ€§
        try:
            text.encode('utf-8').decode('utf-8')
            score += 0.3
        except UnicodeError:
            pass
        
        # 2. æ£€æŸ¥å¸¸è§ç¼–ç çš„å…¼å®¹æ€§
        for encoding in ['gbk', 'gb2312', 'utf-16']:
            try:
                text.encode(encoding).decode(encoding)
                score += 0.2
                break
            except UnicodeError:
                pass
        
        # 3. æ£€æŸ¥æ§åˆ¶å­—ç¬¦æ¯”ä¾‹
        control_chars = sum(1 for c in text if ord(c) < 32 and c not in '\n\t\r')
        control_ratio = control_chars / len(text)
        if control_ratio < 0.05:  # æ§åˆ¶å­—ç¬¦å°‘äº5%
            score += 0.3
        
        # 4. æ£€æŸ¥å­—ç¬¦åˆ†å¸ƒåˆç†æ€§
        printable_chars = sum(1 for c in text if c.isprintable() or c in '\n\t\r')
        printable_ratio = printable_chars / len(text)
        score += printable_ratio * 0.2
        
        return min(score, 1.0)
    
    def _calculate_readability_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬å¯è¯»æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰"""
        if not text:
            return 0.0
        
        import re
        
        # è®¡ç®—å„ç§å­—ç¬¦ç±»å‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]{2,}\b', text))
        digits = len(re.findall(r'\d', text))
        symbols = len(re.findall(r'[\u3000-\u303f\uff00-\uffef]', text))  # ä¸­æ–‡æ ‡ç‚¹
        
        # è®¡ç®—å¯è¯»å†…å®¹æ¯”ä¾‹
        readable_content = chinese_chars + english_words * 3 + digits * 0.5 + symbols * 0.8
        readability_ratio = readable_content / len(text)
        
        # æŠ€æœ¯å†…å®¹åŠ æˆ
        technical_bonus = 0
        if re.search(r'(ç”µå‹|ç”µæµ|åŠŸç‡|é¢‘ç‡|æµ‹è¯•|ä¿æŠ¤|ç»§ç”µ|è£…ç½®|è®¾å¤‡|æ€§èƒ½|ç²¾åº¦|è§„æ ¼|å‚æ•°|å‹å·)', text):
            technical_bonus += 0.1
        if re.search(r'\d+[A-Za-z%Â°â„ƒÎ©]+', text):  # æ•°å­—+å•ä½
            technical_bonus += 0.1
        
        return min(readability_ratio + technical_bonus, 1.0)
    
    def _has_technical_content(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æŠ€æœ¯å†…å®¹"""
        import re
        
        # æŠ€æœ¯å…³é”®è¯æ¨¡å¼
        tech_patterns = [
            r'(ç”µå‹|ç”µæµ|åŠŸç‡|é¢‘ç‡|ç”µé˜»|ç”µå®¹|ç”µæ„Ÿ)',
            r'(æµ‹è¯•|æµ‹é‡|æ ¡éªŒ|æ£€æµ‹|ç›‘æ§)',
            r'(ä¿æŠ¤|ç»§ç”µ|å®‰å…¨|å¯é )',
            r'(è£…ç½®|è®¾å¤‡|ä»ªå™¨|ä»ªè¡¨)',
            r'(æ€§èƒ½|ç²¾åº¦|èŒƒå›´|è§„æ ¼)',
            r'(é¢å®š|æœ€å¤§|æœ€å°|æ ‡å‡†)',
            r'\d+[A-Za-z%Â°â„ƒÎ©VAKW]+',  # æ•°å­—+å•ä½
            r'[A-Z][A-Z0-9-]{2,}',  # äº§å“å‹å·
        ]
        
        for pattern in tech_patterns:
            if re.search(pattern, text):
                return True
        
        return False
    
    def _enhanced_corruption_detection(self, text: str, file_type: str = '') -> bool:
        """å¢å¼ºçš„ä¹±ç æ£€æµ‹ç®—æ³•"""
        if not text or len(text.strip()) < 20:
            return True
        
        text_sample = text[:3000]  # æ‰©å¤§æ£€æµ‹æ ·æœ¬
        
        # 1. æ™ºèƒ½ç¼–ç æ£€æµ‹å’ŒéªŒè¯
        encoding_score = self._calculate_encoding_score(text_sample)
        if encoding_score < 0.3:  # ç¼–ç è´¨é‡è¿‡ä½
            logger.debug(f"ç¼–ç è´¨é‡è¯„åˆ†è¿‡ä½: {encoding_score}")
            return True
        
        # 2. æ£€æµ‹å¼‚å¸¸é«˜é¢‘çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        corruption_chars = sum(1 for c in text_sample if ord(c) > 0xFFFF or c in 'æ€é‡¨å¨é†¢è— ä¿¹ç‰æ…µæ¥´æ¹¯ç•±ç‘¡æ½©åæ½—æ‘²æ½„å€ç‘©æ•¬')
        if corruption_chars / len(text_sample) > 0.25:  # é™ä½é˜ˆå€¼ï¼Œå‡å°‘è¯¯åˆ¤
            logger.debug(f"æ£€æµ‹åˆ°é«˜é¢‘å¼‚å¸¸å­—ç¬¦: {corruption_chars}/{len(text_sample)}")
            return True
        
        # 3. æ£€æµ‹é‡å¤æ¨¡å¼ï¼ˆä¼˜åŒ–ç®—æ³•ï¼‰
        import re
        # æ›´ç²¾ç¡®çš„é‡å¤æ¨¡å¼æ£€æµ‹
        repeated_patterns = re.findall(r'(.{2,4})\1{6,}', text_sample)  # æé«˜é‡å¤é˜ˆå€¼
        if len(repeated_patterns) > 12:  # æé«˜é‡å¤æ¨¡å¼é˜ˆå€¼
            logger.debug(f"æ£€æµ‹åˆ°å¤§é‡é‡å¤æ¨¡å¼: {len(repeated_patterns)}")
            return True
        
        # 4. æ™ºèƒ½å¯è¯»æ€§åˆ†æ
        readability_score = self._calculate_readability_score(text_sample)
        if readability_score < 0.08:  # è¿›ä¸€æ­¥é™ä½å¯è¯»æ€§é˜ˆå€¼
            logger.debug(f"å¯è¯»æ€§è¯„åˆ†è¿‡ä½: {readability_score}")
            return True
        
        # 5. æŠ€æœ¯å†…å®¹å­˜åœ¨æ€§æ£€æŸ¥
        has_technical_content = self._has_technical_content(text_sample)
        if not has_technical_content and readability_score < 0.15:
            logger.debug("æ— æŠ€æœ¯å†…å®¹ä¸”å¯è¯»æ€§å·®")
            return True
        
        logger.debug(f"âœ… å¢å¼ºæ–‡æœ¬è´¨é‡æ£€æµ‹é€šè¿‡ - ç¼–ç : {encoding_score:.3f}, å¯è¯»æ€§: {readability_score:.3f}")
        return False
    
    def _evaluate_text_quality(self, text: str) -> float:
        """è¯„ä¼°æå–æ–‡æœ¬çš„è´¨é‡ï¼ˆ0-1åˆ†æ•°ï¼‰"""
        if not text or len(text.strip()) < self.min_text_length:
            return 0.0
        
        text_clean = text.strip()
        
        # ğŸ” ä¹±ç æ£€æµ‹ - å¦‚æœæ£€æµ‹åˆ°ä¸¥é‡ä¹±ç ï¼Œç›´æ¥è¿”å›0
        if self._is_corrupted_text(text_clean):
            logger.warning(f"æ£€æµ‹åˆ°ä¹±ç æ–‡æœ¬ï¼Œè´¨é‡è¯„åˆ†ä¸º0")
            return 0.0
        
        score = 0.0
        
        # 1. åŸºç¡€é•¿åº¦åˆ†æ•° (0.3æƒé‡)
        if len(text_clean) > 100:
            length_score = min(len(text_clean) / 1000, 1.0) * 0.3
            score += length_score
        
        # 2. ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ (0.2æƒé‡)
        chinese_chars = len([c for c in text_clean if '\u4e00' <= c <= '\u9fff'])
        if len(text_clean) > 0:
            chinese_ratio = chinese_chars / len(text_clean)
            # ç”µæ°”è®¾å¤‡æ–‡æ¡£é€šå¸¸åŒ…å«ä¸­æ–‡
            if chinese_ratio > 0.1:
                score += chinese_ratio * 0.2
        
        # 3. æŠ€æœ¯å…³é”®è¯å¯†åº¦ (0.25æƒé‡)
        technical_keywords = [
            'äº§å“', 'å‹å·', 'è§„æ ¼', 'å‚æ•°', 'æŠ€æœ¯', 'ç”µå‹', 'ç”µæµ', 'åŠŸç‡', 
            'é¢‘ç‡', 'æµ‹è¯•', 'ä¿æŠ¤', 'ç»§ç”µ', 'è£…ç½®', 'è®¾å¤‡', 'æ€§èƒ½', 'ç²¾åº¦',
            'product', 'model', 'specification', 'parameter', 'voltage', 
            'current', 'power', 'frequency', 'test', 'protection'
        ]
        
        keyword_count = 0
        text_lower = text_clean.lower()
        for keyword in technical_keywords:
            keyword_count += text_lower.count(keyword.lower())
        
        keyword_density = min(keyword_count / max(len(text_clean.split()), 1), 0.1) * 10
        score += keyword_density * 0.25
        
        # 4. æ•°å­—å’Œå•ä½å­˜åœ¨æ€§ (0.15æƒé‡)
        import re
        number_pattern = r'\d+\.?\d*\s*[A-Za-z/Î©%Â°â„ƒV]+'
        number_matches = len(re.findall(number_pattern, text_clean))
        if number_matches > 0:
            number_score = min(number_matches / 20, 1.0) * 0.15
            score += number_score
        
        # 5. å¯è¯»æ€§æ£€æŸ¥ (0.1æƒé‡)
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šä¹±ç å­—ç¬¦
        printable_chars = len([c for c in text_clean if c.isprintable() or c in '\n\t'])
        if len(text_clean) > 0:
            readability = printable_chars / len(text_clean)
            if readability > 0.8:
                score += 0.1
            elif readability > 0.6:
                score += 0.05
        
        # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
        final_score = min(max(score, 0.0), 1.0)
        
        return final_score
    
    def _evaluate_ocr_quality(self, text: str) -> float:
        """è¯„ä¼°OCRæå–æ–‡æœ¬çš„è´¨é‡ï¼ˆ0-1åˆ†æ•°ï¼‰"""
        if not text or len(text.strip()) < 5:
            return 0.0
        
        text_clean = text.strip()
        score = 0.0
        
        # 1. åŸºç¡€é•¿åº¦åˆ†æ•° (0.2æƒé‡)
        if len(text_clean) > 20:
            length_score = min(len(text_clean) / 200, 1.0) * 0.2
            score += length_score
        
        # 2. å¯è¯»å­—ç¬¦æ¯”ä¾‹ (0.3æƒé‡)
        readable_chars = len([c for c in text_clean if c.isalnum() or c.isspace() or c in '.,;:!?-()[]{}'])
        if len(text_clean) > 0:
            readable_ratio = readable_chars / len(text_clean)
            if readable_ratio > 0.7:
                score += 0.3
            elif readable_ratio > 0.5:
                score += 0.2
            elif readable_ratio > 0.3:
                score += 0.1
        
        # 3. ä¸­æ–‡æˆ–è‹±æ–‡å­—ç¬¦å­˜åœ¨æ€§ (0.25æƒé‡)
        chinese_chars = len([c for c in text_clean if '\u4e00' <= c <= '\u9fff'])
        english_words = len([word for word in text_clean.split() if word.isalpha() and len(word) >= 3])
        
        if chinese_chars > 0 or english_words > 0:
            content_score = min((chinese_chars + english_words * 2) / 50, 1.0) * 0.25
            score += content_score
        
        # 4. æ•°å­—å’ŒæŠ€æœ¯ç‰¹å¾ (0.15æƒé‡)
        import re
        has_numbers = bool(re.search(r'\d', text_clean))
        has_units = bool(re.search(r'\d+\s*[A-Za-z%Â°â„ƒÎ©]', text_clean))
        
        if has_numbers:
            score += 0.1
        if has_units:
            score += 0.05
        
        # 5. é¿å…æ˜æ˜¾çš„OCRé”™è¯¯ (0.1æƒé‡)
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šçš„å•ä¸ªå­—ç¬¦æˆ–æ˜æ˜¾é”™è¯¯
        single_chars = len([word for word in text_clean.split() if len(word) == 1])
        total_words = len(text_clean.split())
        
        if total_words > 0:
            single_char_ratio = single_chars / total_words
            if single_char_ratio < 0.3:  # å•å­—ç¬¦æ¯”ä¾‹ä½äº30%
                score += 0.1
        
        return min(max(score, 0.0), 1.0)
    
    def _contains_garbage_chinese(self, text: str) -> bool:
        """æ£€æµ‹ä¸­æ–‡æ–‡æœ¬æ˜¯å¦åŒ…å«ä¹±ç æ±‰å­—"""
        # å¸¸è§çš„.docè§£æé”™è¯¯äº§ç”Ÿçš„ä¹±ç æ±‰å­—
        garbage_chinese_chars = set([
            'æ½—', 'æ‘²', 'æ¥', 'ç‰£', 'ç¯', 'æ™¯', 'ç……', 'æ…µ', 'æ¥´', 'æ¹¯', 'ç•±', 'ç‘¡', 'æ½©',
            'å', 'æ½—', 'æ‘²', 'æ½„', 'å€', 'ç‘©', 'æ•¬', 'ç‰', 'æ…©', 'è¢ˆ', 'éœ¡', 'è ˆ', 'è¢¢',
            'å±œ', 'å±', 'å±¬', 'å±­', 'å±¨', 'å±ª', 'å±¢', 'å±£', 'å±¤', 'å±¥', 'å±¦', 'å±§', 'å±¨', 'å±©', 'å±²'
        ])
        
        garbage_count = sum(1 for c in text if c in garbage_chinese_chars)
        garbage_ratio = garbage_count / len(text) if text else 0
        
        # å¦‚æœåƒåœ¾æ±‰å­—è¶…è¿‡20%ï¼Œè®¤ä¸ºæ˜¯ä¹±ç 
        return garbage_ratio > 0.2
    
    def _is_meaningful_english(self, text: str) -> bool:
        """æ£€æµ‹è‹±æ–‡æ–‡æœ¬æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆéä¹±ç ï¼‰"""
        import re
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„è‹±æ–‡å•è¯æ¨¡å¼
        words = text.lower().split()
        
        # æŠ€æœ¯æ–‡æ¡£å¸¸è§çš„è‹±æ–‡å•è¯
        common_technical_words = set([
            'the', 'and', 'of', 'to', 'a', 'in', 'for', 'is', 'on', 'with', 'as', 'by',
            'test', 'voltage', 'current', 'power', 'frequency', 'protection', 'relay',
            'device', 'equipment', 'specification', 'parameter', 'technical', 'model',
            'product', 'range', 'accuracy', 'measurement', 'control', 'system',
            'output', 'input', 'signal', 'data', 'interface', 'communication'
        ])
        
        meaningful_words = 0
        for word in words:
            clean_word = re.sub(r'[^a-z]', '', word)  # ç§»é™¤æ ‡ç‚¹
            if (len(clean_word) >= 3 and 
                (clean_word in common_technical_words or 
                 re.match(r'^[a-z]+$', clean_word))):  # çº¯è‹±æ–‡å­—æ¯ç»„æˆ
                meaningful_words += 1
        
        # å¦‚æœæœ‰æ„ä¹‰çš„å•è¯æ¯”ä¾‹è¶…è¿‡60%ï¼Œè®¤ä¸ºæ˜¯æœ‰æ„ä¹‰çš„è‹±æ–‡
        return meaningful_words / max(len(words), 1) > 0.6
    
    def _is_corrupted_tech_data(self, text: str) -> bool:
        """æ£€æµ‹æŠ€æœ¯æ•°æ®æ˜¯å¦æŸå"""
        import re
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„ä¹±ç æ¨¡å¼
        corruption_patterns = [
            r'[æ½—æ‘²æ¥ç‰£ç¯æ™¯ç……æ…µæ¥´æ¹¯ç•±ç‘¡æ½©åæ½—æ‘²æ½„å€ç‘©æ•¬ç‰æ…©è¢ˆéœ¡è ˆè¢¢]',  # ä¹±ç æ±‰å­—
            r'[â–‰â–Šâ–‹â–Œâ–â–â–â–ˆâ–„â–€â– â–¡â–²â–³â–¼â–½â—†â—‡â—‹â—â—â˜†â˜…]',  # å›¾å½¢å­—ç¬¦
            r'[\ue000-\uf8ff]',  # ç§ç”¨åŒºå­—ç¬¦
            r'[ï¿½?]{2,}',         # æ›¿ä»£å­—ç¬¦
        ]
        
        for pattern in corruption_patterns:
            if re.search(pattern, text):
                return True
        
        return False
    
    def _is_obvious_garbage_line(self, line: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæ˜æ˜¾çš„åƒåœ¾è¡Œ"""
        import re
        
        if not line or len(line.strip()) < 2:
            return True
        
        line = line.strip()
        
        # æ£€æµ‹æ˜æ˜¾çš„åƒåœ¾æ¨¡å¼
        garbage_patterns = [
            r'^[æ½—æ‘²æ¥ç‰£ç¯æ™¯ç……æ…µæ¥´æ¹¯ç•±ç‘¡æ½©åæ½—æ‘²æ½„å€ç‘©æ•¬ç‰æ…©è¢ˆéœ¡è ˆè¢¢]+$',  # å…¨ä¹±ç æ±‰å­—
            r'^[â–‰â–Šâ–‹â–Œâ–â–â–â–ˆâ–„â–€â– â–¡â–²â–³â–¼â–½â—†â—‡â—‹â—â—â˜†â˜…]+$',  # å…¨å›¾å½¢å­—ç¬¦
            r'^[\u0080-\u00ff]{2,}$',         # å…¨é«˜ä½ASCII
            r'^[ï¿½?]{2,}$',                    # å…¨æ›¿ä»£å­—ç¬¦
            r'^(.)\1{5,}$',                   # åŒä¸€å­—ç¬¦é‡å¤6æ¬¡ä»¥ä¸Š
            r'^[\s\-\+\=\|]{3,}$',           # è¡¨æ ¼åˆ†éš”ç¬¦
        ]
        
        for pattern in garbage_patterns:
            if re.search(pattern, line):
                return True
        
        # æ£€æµ‹å¯è¯»æ€§ï¼šå¦‚æœå¯è¯»å­—ç¬¦å°‘äº50%ï¼Œè®¤ä¸ºæ˜¯åƒåœ¾
        readable_chars = sum(1 for c in line if (c.isalnum() or c.isspace() or 
                                                c in '.,;:!?-()[]{}""''ã€ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ'))
        readable_ratio = readable_chars / len(line)
        
        return readable_ratio < 0.5
    
    def _clean_ocr_text(self, text: str) -> str:
        """æ¸…ç†OCRæå–çš„æ–‡æœ¬"""
        import re
        
        # 1. ç§»é™¤æ˜æ˜¾çš„OCRé”™è¯¯å­—ç¬¦
        # ç§»é™¤å­¤ç«‹çš„å¥‡æ€ªå­—ç¬¦
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff.,;:!?\-()[\]{}\'"Â°â„ƒ%Î©/\\]', ' ', text)
        
        # 2. è§„èŒƒç©ºç™½å­—ç¬¦
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)
        
        # 3. ç§»é™¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯OCRå™ªéŸ³ï¼‰
        lines = cleaned.split('\n')
        filtered_lines = []
        
        for line in lines:
            line = line.strip()
            # ä¿ç•™è¾ƒé•¿çš„è¡Œæˆ–åŒ…å«æ•°å­—/æŠ€æœ¯ä¿¡æ¯çš„è¡Œ
            if len(line) >= 3 or re.search(r'\d|[A-Za-z]{3,}|[\u4e00-\u9fff]{2,}', line):
                filtered_lines.append(line)
        
        # 4. é‡æ–°ç»„åˆ
        result = '\n'.join(filtered_lines)
        
        # 5. æœ€ç»ˆæ¸…ç†
        result = result.strip()
        
        return result