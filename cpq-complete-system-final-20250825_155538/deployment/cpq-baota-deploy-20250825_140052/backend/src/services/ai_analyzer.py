# -*- coding: utf-8 -*-
"""
AIåˆ†ææœåŠ¡
æ•´åˆæ–‡æ¡£å¤„ç†å’ŒOpenAIåˆ†æï¼Œæä¾›å®Œæ•´çš„AIäº§å“åˆ†æåŠŸèƒ½
"""
import logging
import time
from typing import Dict, Any, Optional, Tuple
from werkzeug.datastructures import FileStorage

from .document_processor import DocumentProcessor
from .zhipuai_client import ZhipuAIClient
from .learning_engine import LearningEngine
from .table_parser import TableParser
from .confidence_scorer import ConfidenceScorer

logger = logging.getLogger(__name__)

class AIAnalyzer:
    """AIäº§å“åˆ†æå™¨"""
    
    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.ai_client = ZhipuAIClient()
        self.learning_engine = LearningEngine()
        self.table_parser = TableParser()
        self.confidence_scorer = ConfidenceScorer()
    
    def analyze_product_document(self, file: FileStorage, user_id: int = None) -> Dict[str, Any]:
        """
        åˆ†æäº§å“æ–‡æ¡£ï¼Œæå–äº§å“ä¿¡æ¯
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            user_id: ç”¨æˆ·IDï¼Œç”¨äºä¸ªæ€§åŒ–åˆ†æ
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æç»“æœ
        """
        start_time = time.time()
        
        try:
            # 1. å¤„ç†æ–‡æ¡£ï¼Œæå–æ–‡æœ¬
            logger.info(f"Processing document: {file.filename}")
            text_content, doc_info = self.document_processor.process_document(file)
            
            if not text_content.strip():
                raise ValueError("Document contains no readable text content")
            
            # ğŸ” å¢å¼ºä¹±ç æ£€æµ‹ - ä½¿ç”¨æ–‡æ¡£å¤„ç†å™¨çš„å¢å¼ºç®—æ³•
            if self.document_processor._enhanced_corruption_detection(text_content, doc_info.get('type', '')):
                error_msg = self._generate_corruption_error_message(file.filename, doc_info)
                raise ValueError(error_msg)
            
            # 2. å¢å¼ºè¡¨æ ¼è§£æ
            logger.info(f"Enhancing extraction with table parsing")
            
            # 3. è·å–ä¸ªæ€§åŒ–æç¤ºï¼ˆå¦‚æœæœ‰ç”¨æˆ·IDï¼‰
            personalized_hints = {}
            if user_id:
                personalized_hints = self.learning_engine.get_personalized_hints(
                    user_id=user_id,
                    document_type=doc_info.get('type', 'unknown'),
                    extracted_data={}
                )
            
            # 4. ä½¿ç”¨AIåˆ†ææ–‡æœ¬ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„åˆ†å±‚åˆ†ææ–¹æ³•ï¼‰
            logger.info(f"Analyzing document content with AI (length: {len(text_content)} chars)")
            ai_result = self.ai_client.analyze_product_document(
                document_content=text_content,
                document_name=file.filename or "unknown"
            )
            
            # 5. ä½¿ç”¨è¡¨æ ¼è§£æå™¨å¢å¼ºç»“æœ
            enhanced_result = self.table_parser.enhance_extraction_with_tables(ai_result, text_content)
            
            # 6. è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
            confidence_scores = self.confidence_scorer.calculate_comprehensive_confidence(
                extracted_data=enhanced_result,
                document_info=doc_info,
                historical_context=personalized_hints.get('pattern_context') if user_id else None
            )
            
            # 7. è®¡ç®—åˆ†ææ—¶é•¿
            analysis_duration = int(time.time() - start_time)
            
            # 8. æ™ºèƒ½ä¿®å¤äº§å“åç§°ï¼ˆå¦‚æœè§„æ ¼æå–æˆåŠŸä½†åç§°ä¸ºç©ºï¼‰
            current_name = enhanced_result.get('basic_info', {}).get('name', '')
            logger.info(f"ğŸ” æ£€æŸ¥äº§å“åç§°ä¿®å¤ - å½“å‰åç§°: '{current_name}', æœ‰è§„æ ¼å‚æ•°: {bool(enhanced_result.get('specifications'))}")
            
            if not current_name.strip() and enhanced_result.get('specifications'):
                logger.info(f"ğŸ”§ å¼€å§‹ä¿®å¤ç¼ºå¤±çš„äº§å“åç§°ï¼Œæ–‡ä»¶å: {file.filename}")
                enhanced_result = self._fix_missing_product_name(enhanced_result, file.filename or "unknown")
                new_name = enhanced_result.get('basic_info', {}).get('name', '')
                logger.info(f"âœ… äº§å“åç§°ä¿®å¤å®Œæˆ: '{new_name}'")
            
            # ğŸ”§ 9. æœ€ç»ˆæ•°æ®æ¸…ç† - ç¡®ä¿æ‰€æœ‰è§„æ ¼å‚æ•°éƒ½æ˜¯æœ‰æ•ˆçš„
            if enhanced_result.get('specifications'):
                cleaned_specs = self.table_parser._clean_specification_data(enhanced_result['specifications'])
                enhanced_result['specifications'] = cleaned_specs
                logger.info(f"æœ€ç»ˆè§„æ ¼å‚æ•°æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(cleaned_specs)} é¡¹æœ‰æ•ˆå‚æ•°")
            
            # ğŸ”§ 10. æœ€ç»ˆæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            final_basic_info = enhanced_result.get('basic_info', {})
            final_specs = enhanced_result.get('specifications', {})
            
            logger.info(f"ğŸ“Š æœ€ç»ˆæ•°æ®æ£€æŸ¥:")
            logger.info(f"   - äº§å“åç§°: '{final_basic_info.get('name', '')}' (é•¿åº¦: {len(final_basic_info.get('name', ''))})")
            logger.info(f"   - äº§å“ä»£ç : '{final_basic_info.get('code', '')}' (é•¿åº¦: {len(final_basic_info.get('code', ''))})")
            logger.info(f"   - äº§å“åˆ†ç±»: '{final_basic_info.get('category', '')}' (é•¿åº¦: {len(final_basic_info.get('category', ''))})")
            logger.info(f"   - è§„æ ¼å‚æ•°: {len(final_specs)} é¡¹")
            logger.info(f"   - åŸºç¡€ä¿¡æ¯å®Œæ•´ç»“æ„: {list(final_basic_info.keys())}")
            
            # å¦‚æœå…³é”®ä¿¡æ¯ç¼ºå¤±ï¼Œå°è¯•ä»ç½®ä¿¡åº¦è¯„åˆ†æˆ–å…¶ä»–æ¥æºæ¢å¤
            if not final_basic_info.get('name', '').strip():
                logger.warning("âš ï¸ äº§å“åç§°ä¸ºç©ºï¼Œå°è¯•ä»åˆ†æä¸Šä¸‹æ–‡æ¢å¤")
                # å¯ä»¥ä»confidence_scoresæˆ–å…¶ä»–åœ°æ–¹å°è¯•æ¢å¤æ•°æ®
            
            # 11. æ„å»ºå®Œæ•´ç»“æœ
            result = {
                'success': True,
                'document_info': {
                    **doc_info,
                    'analysis_duration': analysis_duration
                },
                'extracted_data': enhanced_result,
                'confidence_scores': confidence_scores,
                'personalized_hints': personalized_hints.get('personalized_hints', []) if user_id else [],
                'predicted_modifications': personalized_hints.get('predicted_modifications', {}) if user_id else {},
                'text_preview': text_content[:500] + "..." if len(text_content) > 500 else text_content,
                'analysis_timestamp': int(time.time())
            }
            
            logger.info(f"Document analysis completed successfully in {analysis_duration}s "
                       f"(confidence: {confidence_scores.get('overall', 'N/A')})")
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"AI analysis failed for {file.filename}: {error_msg}")
            
            # ğŸ”§ å¢å¼ºé”™è¯¯åˆ†ç±»å’Œå¤„ç†
            error_type = self._classify_error_type(e, error_msg)
            detailed_error = self._generate_detailed_error_message(e, error_type, file.filename, doc_info if 'doc_info' in locals() else {})
            
            return {
                'success': False,
                'error': detailed_error['message'],
                'error_type': error_type,
                'error_details': detailed_error['details'],
                'suggestions': detailed_error['suggestions'],
                'document_info': {
                    'filename': file.filename,
                    'analysis_duration': int(time.time() - start_time),
                    'file_type': getattr(file, 'mimetype', 'unknown')
                }
            }
    
    def _fix_missing_product_name(self, extracted_data: Dict[str, Any], filename: str) -> Dict[str, Any]:
        """æ™ºèƒ½ä¿®å¤ç¼ºå¤±çš„äº§å“åç§°"""
        try:
            basic_info = extracted_data.get('basic_info', {})
            specifications = extracted_data.get('specifications', {})
            
            # ä»æ–‡ä»¶åæ¨æ–­äº§å“åç§°
            if 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª' in filename or 'ç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª' in filename:
                basic_info['name'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª'
                basic_info['category'] = 'ç»§ç”µä¿æŠ¤æµ‹è¯•è®¾å¤‡'
                basic_info['description'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ªï¼Œç”¨äºç”µåŠ›ç³»ç»Ÿç»§ç”µä¿æŠ¤è£…ç½®çš„å…¨é¢æµ‹è¯•å’Œæ ¡éªŒ'
                
            elif 'å˜å‹å™¨' in filename:
                basic_info['name'] = 'ç”µåŠ›å˜å‹å™¨'
                basic_info['category'] = 'å˜å‹å™¨è®¾å¤‡'
                
            elif 'å¼€å…³' in filename:
                basic_info['name'] = 'ç”µåŠ›å¼€å…³è®¾å¤‡'
                basic_info['category'] = 'å¼€å…³è®¾å¤‡'
                
            else:
                # ä»è§„æ ¼å‚æ•°æ¨æ–­äº§å“ç±»å‹
                if any('ç›¸' in str(key) for key in specifications.keys()):
                    if '6' in str(specifications) or 'å…­' in str(specifications):
                        basic_info['name'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª'
                        basic_info['category'] = 'ç»§ç”µä¿æŠ¤æµ‹è¯•è®¾å¤‡'
                        basic_info['description'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ªï¼Œç”¨äºç”µåŠ›ç³»ç»Ÿç»§ç”µä¿æŠ¤è£…ç½®çš„æµ‹è¯•'
                    else:
                        basic_info['name'] = 'ç”µåŠ›æµ‹è¯•è®¾å¤‡'
                        basic_info['category'] = 'ç”µåŠ›æµ‹è¯•è®¾å¤‡'
                        
                elif any('å˜å‹å™¨' in str(key) for key in specifications.keys()):
                    basic_info['name'] = 'ç”µåŠ›å˜å‹å™¨'
                    basic_info['category'] = 'å˜å‹å™¨è®¾å¤‡'
                    
                else:
                    # é€šç”¨ç”µæ°”è®¾å¤‡
                    basic_info['name'] = 'ç”µåŠ›è®¾å¤‡'
                    basic_info['category'] = 'ç”µåŠ›è®¾å¤‡'
            
            # æ›´æ–°ç½®ä¿¡åº¦
            confidence = extracted_data.get('confidence', {})
            if basic_info.get('name'):
                confidence['basic_info'] = max(confidence.get('basic_info', 0), 0.8)
                confidence['overall'] = max(confidence.get('overall', 0), 
                                          (confidence.get('basic_info', 0.8) + confidence.get('specifications', 0.7)) / 2)
            
            extracted_data['basic_info'] = basic_info
            extracted_data['confidence'] = confidence
            
            logger.info(f"æ™ºèƒ½ä¿®å¤äº§å“åç§°: {basic_info.get('name', 'N/A')}")
            
            return extracted_data
            
        except Exception as e:
            logger.error(f"ä¿®å¤äº§å“åç§°å¤±è´¥: {str(e)}")
            return extracted_data
    
    def _is_text_corrupted(self, text: str, file_extension: str = '') -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸¥é‡ä¹±ç ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ"""
        if not text or len(text.strip()) < 20:  # ğŸ”§ è¿›ä¸€æ­¥é™ä½æœ€å°é•¿åº¦è¦æ±‚ï¼ŒOCRæ–‡æœ¬å¯èƒ½å¾ˆçŸ­
            return True
        
        # ğŸ–¼ï¸ ç‰¹æ®Šå¤„ç†å›¾ç‰‡æ–‡ä»¶çš„OCRæ–‡æœ¬
        is_image_ocr = file_extension.lower() in ['.png', '.jpg', '.jpeg', '.gif', '.bmp']
        
        text_sample = text[:3000]  # å–å‰3000å­—ç¬¦è¿›è¡Œæ£€æµ‹
        
        # ğŸ” ä¹±ç ç‰¹å¾æ£€æµ‹
        import re
        
        # 1. æ£€æµ‹æ˜æ˜¾çš„ç¼–ç é”™è¯¯å­—ç¬¦ - å¯¹å›¾ç‰‡OCRæ›´å®½æ¾
        corruption_chars = sum(1 for c in text_sample if c in 'æ€é‡¨å¨é†¢è— ä¿¹ç‰æ…µæ¥´æ¹¯ç•±ç‘¡æ½©åæ½—æ‘²æ½„å€ç‘©æ•¬æ¥ç‰£ç¯æ™¯ç……æ…µæ¥´æ¹¯')
        corruption_threshold = 0.5 if is_image_ocr else 0.3  # ğŸ–¼ï¸ å›¾ç‰‡OCRå…è®¸æ›´å¤šé”™è¯¯å­—ç¬¦
        if corruption_chars / len(text_sample) > corruption_threshold:
            return True
            
        # 2. æ£€æµ‹é‡å¤æ¨¡å¼è¿‡å¤š - å¯¹å›¾ç‰‡OCRæ›´å®½æ¾
        repeated_patterns = re.findall(r'(.{2,4})\1{5,}', text_sample)
        repeat_threshold = 25 if is_image_ocr else 15  # ğŸ–¼ï¸ å›¾ç‰‡OCRå…è®¸æ›´å¤šé‡å¤æ¨¡å¼
        if len(repeated_patterns) > repeat_threshold:
            return True
            
        # 3. æ£€æµ‹å¯è¯»æ€§ - å¯è¯»çš„ä¸­æ–‡æˆ–è‹±æ–‡å†…å®¹å¤ªå°‘
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text_sample))
        english_words = len(re.findall(r'\b[a-zA-Z]{2,}\b', text_sample))  # ğŸ”§ é™ä½è‹±æ–‡å•è¯é•¿åº¦è¦æ±‚
        digits = len(re.findall(r'\d', text_sample))
        symbols = len(re.findall(r'[A-Za-z]\d+|\d+[A-Za-z]', text_sample))  # ğŸ”§ æ·»åŠ å­—æ¯æ•°å­—ç»„åˆæ£€æµ‹
        
        # è®¡ç®—å¯è¯»å†…å®¹æ¯”ä¾‹ - å¯¹å›¾ç‰‡OCRæ›´å®½æ¾
        readable_content = chinese_chars + english_words * 2 + digits * 0.8 + symbols * 1.5  # ğŸ–¼ï¸ è°ƒæ•´æƒé‡ï¼Œé‡è§†æ•°å­—å’Œç¬¦å·
        readable_ratio = readable_content / len(text_sample)
        
        # ğŸ”§ é’ˆå¯¹ä¸åŒæ–‡ä»¶ç±»å‹çš„ç‰¹æ®Šæ£€æµ‹
        # æ£€æµ‹æ˜¯å¦æœ‰æŠ€æœ¯å…³é”®è¯æˆ–æ•°å­—+å•ä½çš„ç»„åˆ
        has_technical_content = bool(re.search(
            r'(ç”µå‹|ç”µæµ|åŠŸç‡|é¢‘ç‡|æµ‹è¯•|ä¿æŠ¤|ç»§ç”µ|è£…ç½®|è®¾å¤‡|æ€§èƒ½|ç²¾åº¦|è§„æ ¼|å‚æ•°|å‹å·|'
            r'voltage|current|power|frequency|test|specification|model|type|parameter)',
            text_sample, re.IGNORECASE
        ))
        has_numeric_units = bool(re.search(r'\d+\s*[A-Za-z%Â°â„ƒÎ©Î©Î¼Î¼Â±]+', text_sample))
        has_product_codes = bool(re.search(r'[A-Z0-9-]{3,}', text_sample))  # ğŸ”§ æ£€æµ‹äº§å“ç¼–ç æ¨¡å¼
        
        # ğŸ–¼ï¸ å›¾ç‰‡OCRçš„ç‰¹æ®Šæ£€æµ‹ - æ›´å®½æ¾çš„åˆ¤æ–­
        if is_image_ocr:
            # å¦‚æœæœ‰ä»»ä½•æŠ€æœ¯å†…å®¹ã€æ•°å­—å•ä½æˆ–äº§å“ç¼–ç ï¼Œæå¤§é™ä½å¯è¯»æ€§è¦æ±‚
            if has_technical_content or has_numeric_units or has_product_codes:
                min_readable_ratio = 0.02  # ğŸ–¼ï¸ æä½é˜ˆå€¼ï¼Œåªè¦æœ‰ä¸€ç‚¹æŠ€æœ¯å†…å®¹å°±æ¥å—
            else:
                min_readable_ratio = 0.03  # ğŸ–¼ï¸ ç¨ä½é˜ˆå€¼
        else:
            # æ™®é€šæ–‡æ¡£çš„æ£€æµ‹
            if has_technical_content or has_numeric_units:
                min_readable_ratio = 0.05
            else:
                min_readable_ratio = 0.08
        
        if readable_ratio < min_readable_ratio:
            logger.debug(f"ğŸ“„ æ–‡æœ¬å¯è¯»æ€§æ£€æµ‹ - æ–‡ä»¶ç±»å‹: {'å›¾ç‰‡OCR' if is_image_ocr else 'æ™®é€šæ–‡æ¡£'}")
            logger.debug(f"âŒ å¯è¯»æ€§ä¸è¶³: {readable_ratio:.3f} < {min_readable_ratio} "
                        f"(ä¸­æ–‡:{chinese_chars}, è‹±æ–‡:{english_words}, æ•°å­—:{digits}, ç¬¦å·:{symbols})")
            logger.debug(f"ğŸ” æŠ€æœ¯å†…å®¹: {has_technical_content}, æ•°å­—å•ä½: {has_numeric_units}, äº§å“ç¼–ç : {has_product_codes}")
            return True
        
        logger.debug(f"âœ… æ–‡æœ¬è´¨é‡æ£€æµ‹é€šè¿‡ - å¯è¯»æ€§: {readable_ratio:.3f} >= {min_readable_ratio}")
        return False
    
    def _generate_corruption_error_message(self, filename: str, doc_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¹±ç é”™è¯¯çš„è¯¦ç»†ä¿¡æ¯"""
        file_type = doc_info.get('type', 'unknown')
        file_size = doc_info.get('size', 0)
        
        error_msg = f"æ–‡æ¡£åˆ†æå¤±è´¥ï¼šæ— æ³•ä»æ–‡ä»¶ '{filename}' ä¸­æå–å¯è¯»æ–‡æœ¬å†…å®¹\n\n"
        
        # ğŸ” å…·ä½“è¯Šæ–­ä¿¡æ¯
        error_msg += "é—®é¢˜è¯Šæ–­ï¼š\n"
        error_msg += f"â€¢ æ–‡ä»¶ç±»å‹: {file_type.upper()}\n"
        error_msg += f"â€¢ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚\n"
        error_msg += "â€¢ æ£€æµ‹åˆ°æ–‡æœ¬ç¼–ç é—®é¢˜æˆ–æ–‡ä»¶æ ¼å¼å¼‚å¸¸\n\n"
        
        # ğŸ› ï¸ è§£å†³å»ºè®®
        error_msg += "å»ºè®®çš„è§£å†³æ–¹æ¡ˆï¼š\n"
        
        if file_type == 'doc':
            error_msg += "1. å°†æ–‡ä»¶è½¬æ¢ä¸º .docx æ ¼å¼ï¼ˆæ¨èï¼‰\n"
            error_msg += "2. ä½¿ç”¨Microsoft Wordæ‰“å¼€æ–‡ä»¶ï¼Œå¦å­˜ä¸ºè¾ƒæ–°çš„æ ¼å¼\n"
            error_msg += "3. ç¡®è®¤æ–‡ä»¶æœªæŸåä¸”åŒ…å«æ–‡æœ¬å†…å®¹ï¼ˆè€Œéçº¯å›¾ç‰‡ï¼‰\n"
            error_msg += "4. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å¯†ç ä¿æŠ¤æˆ–è®¿é—®é™åˆ¶\n"
        elif file_type == 'pdf':
            error_msg += "1. ç¡®è®¤PDFåŒ…å«å¯é€‰æ‹©çš„æ–‡æœ¬ï¼ˆè€Œéæ‰«æå›¾ç‰‡ï¼‰\n"
            error_msg += "2. å°†PDFè½¬æ¢ä¸ºWordæ–‡æ¡£æ ¼å¼\n"
            error_msg += "3. ä½¿ç”¨OCRå·¥å…·è½¬æ¢æ‰«æç‰ˆPDF\n"
        else:
            error_msg += "1. æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œç¼–ç æ˜¯å¦æ­£ç¡®\n"
            error_msg += "2. å°è¯•ä½¿ç”¨å…¶ä»–è½¯ä»¶é‡æ–°ä¿å­˜æ–‡ä»¶\n"
            error_msg += "3. ç¡®è®¤æ–‡ä»¶åŒ…å«å¯è¯»çš„æ–‡æœ¬å†…å®¹\n"
            
        error_msg += "\nå¦‚é—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒå¹¶æä¾›æ–‡ä»¶æ ·æœ¬ã€‚"
        
        return error_msg
    
    def _classify_error_type(self, exception: Exception, error_msg: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_msg_lower = error_msg.lower()
        
        # æ–‡æ¡£å¤„ç†ç›¸å…³é”™è¯¯
        if "encoding" in error_msg_lower or "decode" in error_msg_lower or "æ— æ³•ä»æ–‡ä»¶ä¸­æå–å¯è¯»æ–‡æœ¬" in error_msg:
            return "encoding_error"
        elif "file size" in error_msg_lower or "exceeds limit" in error_msg_lower:
            return "file_size_error"
        elif "unsupported file format" in error_msg_lower or "format" in error_msg_lower:
            return "format_error"
        elif "no text content" in error_msg_lower or "empty" in error_msg_lower:
            return "empty_content_error"
        elif "corrupted" in error_msg_lower or "ä¹±ç " in error_msg:
            return "corruption_error"
        
        # AIæœåŠ¡ç›¸å…³é”™è¯¯
        elif "api" in error_msg_lower and ("timeout" in error_msg_lower or "connection" in error_msg_lower):
            return "ai_service_timeout"
        elif "api" in error_msg_lower and ("rate limit" in error_msg_lower or "quota" in error_msg_lower):
            return "ai_service_quota"
        elif "api" in error_msg_lower and "key" in error_msg_lower:
            return "ai_service_auth"
        elif "ai" in error_msg_lower or "openai" in error_msg_lower or "zhipu" in error_msg_lower:
            return "ai_service_error"
        
        # ç³»ç»Ÿèµ„æºé”™è¯¯
        elif "memory" in error_msg_lower or "ram" in error_msg_lower:
            return "memory_error"
        elif "disk" in error_msg_lower or "space" in error_msg_lower:
            return "disk_error"
        elif "timeout" in error_msg_lower:
            return "timeout_error"
        
        # æƒé™é”™è¯¯
        elif "permission" in error_msg_lower or "access" in error_msg_lower:
            return "permission_error"
        
        # æœªçŸ¥é”™è¯¯
        else:
            return "unknown_error"
    
    def _generate_detailed_error_message(self, exception: Exception, error_type: str, 
                                       filename: str, doc_info: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯"""
        base_error = str(exception)
        
        error_messages = {
            "encoding_error": {
                "title": "æ–‡æ¡£ç¼–ç é—®é¢˜",
                "message": f"æ— æ³•æ­£ç¡®è¯»å–æ–‡æ¡£ '{filename}' çš„æ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜",
                "details": [
                    "æ£€æµ‹åˆ°æ–‡æ¡£ç¼–ç æ ¼å¼ä¸å…¼å®¹æˆ–æ–‡ä»¶æŸå",
                    "æ–‡æ¡£å¯èƒ½ä½¿ç”¨äº†ä¸æ”¯æŒçš„å­—ç¬¦ç¼–ç ",
                    "æ–‡ä»¶å†…å®¹å¯èƒ½åŒ…å«ä¹±ç æˆ–ç‰¹æ®Šå­—ç¬¦"
                ],
                "suggestions": [
                    "å°è¯•å°†æ–‡æ¡£è½¬æ¢ä¸ºUTF-8ç¼–ç æ ¼å¼",
                    "å°†.docæ–‡ä»¶è½¬æ¢ä¸º.docxæ ¼å¼",
                    "ä½¿ç”¨Microsoft Wordé‡æ–°ä¿å­˜æ–‡æ¡£",
                    "ç¡®è®¤æ–‡æ¡£å®Œæ•´ä¸”æœªæŸå"
                ]
            },
            "format_error": {
                "title": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼",
                "message": f"æ–‡ä»¶ '{filename}' çš„æ ¼å¼ä¸å—æ”¯æŒ",
                "details": [
                    f"æ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»å‹: {doc_info.get('mimetype', 'æœªçŸ¥')}",
                    "ç³»ç»Ÿåªæ”¯æŒå¸¸è§çš„åŠå…¬æ–‡æ¡£æ ¼å¼",
                    "è¯·ç¡®è®¤æ–‡ä»¶æ‰©å±•åä¸å®é™…å†…å®¹åŒ¹é…"
                ],
                "suggestions": [
                    "æ”¯æŒæ ¼å¼: PDF, DOCX, DOC, XLSX, XLS, PPTX, TXT",
                    "å°†æ–‡ä»¶è½¬æ¢ä¸ºæ”¯æŒçš„æ ¼å¼",
                    "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸‹è½½",
                    "ç¡®è®¤æ–‡ä»¶æœªè¢«åŠ å¯†æˆ–å¯†ç ä¿æŠ¤"
                ]
            },
            "file_size_error": {
                "title": "æ–‡ä»¶å¤§å°è¶…é™",
                "message": f"æ–‡ä»¶ '{filename}' å¤§å°è¶…å‡ºç³»ç»Ÿé™åˆ¶",
                "details": [
                    f"å½“å‰æ–‡ä»¶å¤§å°: {doc_info.get('size', 0) / 1024 / 1024:.1f}MB",
                    "ç³»ç»Ÿé™åˆ¶: 10MB",
                    "å¤§æ–‡ä»¶å¯èƒ½å½±å“å¤„ç†æ€§èƒ½å’Œç¨³å®šæ€§"
                ],
                "suggestions": [
                    "å‹ç¼©æ–‡æ¡£æˆ–å‡å°‘å†…å®¹",
                    "åˆ†å‰²å¤§æ–‡æ¡£ä¸ºå¤šä¸ªå°æ–‡æ¡£",
                    "ç§»é™¤æ–‡æ¡£ä¸­çš„å¤§å›¾ç‰‡æˆ–åª’ä½“æ–‡ä»¶",
                    "ä½¿ç”¨PDFæ ¼å¼å¯ä»¥æœ‰æ•ˆå‡å°æ–‡ä»¶å¤§å°"
                ]
            },
            "empty_content_error": {
                "title": "æ–‡æ¡£å†…å®¹ä¸ºç©º",
                "message": f"æ–‡æ¡£ '{filename}' ä¸­æ²¡æœ‰å‘ç°å¯è¯»çš„æ–‡æœ¬å†…å®¹",
                "details": [
                    "æ–‡æ¡£å¯èƒ½åªåŒ…å«å›¾ç‰‡æˆ–å›¾è¡¨",
                    "æ–‡æ¡£å¯èƒ½æ˜¯æ‰«æç‰ˆæœ¬",
                    "æ–‡æ¡£å¯èƒ½å­˜åœ¨æ ¼å¼é—®é¢˜"
                ],
                "suggestions": [
                    "ç¡®è®¤æ–‡æ¡£åŒ…å«æ–‡æœ¬å†…å®¹è€Œéçº¯å›¾ç‰‡",
                    "å¯¹äºæ‰«ææ–‡æ¡£ï¼Œè¯·ä½¿ç”¨OCRè½¯ä»¶è½¬æ¢",
                    "æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ­£ç¡®æ‰“å¼€",
                    "å°è¯•é‡æ–°åˆ›å»ºæˆ–å¯¼å‡ºæ–‡æ¡£"
                ]
            },
            "corruption_error": {
                "title": "æ–‡æ¡£å†…å®¹æŸå",
                "message": f"æ–‡æ¡£ '{filename}' çš„å†…å®¹å‡ºç°æŸåæˆ–ä¸¥é‡ä¹±ç ",
                "details": [
                    "æ£€æµ‹åˆ°å¤§é‡ä¸å¯è¯»å­—ç¬¦",
                    "æ–‡æ¡£å¯èƒ½åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­æŸå",
                    "æ–‡æ¡£ç¼–ç ä¸å®é™…å†…å®¹ä¸åŒ¹é…"
                ],
                "suggestions": [
                    "é‡æ–°ä¸‹è½½æˆ–è·å–åŸå§‹æ–‡æ¡£",
                    "ä½¿ç”¨åŸå§‹è½¯ä»¶é‡æ–°ä¿å­˜æ–‡æ¡£",
                    "æ£€æŸ¥æ–‡æ¡£çš„å®Œæ•´æ€§",
                    "è”ç³»æ–‡æ¡£æä¾›æ–¹ç¡®è®¤æ–‡æ¡£çŠ¶æ€"
                ]
            },
            "ai_service_timeout": {
                "title": "AIæœåŠ¡è¶…æ—¶",
                "message": "AIåˆ†ææœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                "details": [
                    "AIæœåŠ¡å½“å‰è´Ÿè½½è¾ƒé«˜",
                    "ç½‘ç»œè¿æ¥å¯èƒ½ä¸ç¨³å®š",
                    "æ–‡æ¡£å†…å®¹å¯èƒ½è¿‡äºå¤æ‚"
                ],
                "suggestions": [
                    "ç¨åé‡æ–°å°è¯•åˆ†æ",
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€",
                    "å°è¯•åˆ†æè¾ƒç®€å•çš„æ–‡æ¡£",
                    "è”ç³»ç³»ç»Ÿç®¡ç†å‘˜æ£€æŸ¥æœåŠ¡çŠ¶æ€"
                ]
            },
            "ai_service_quota": {
                "title": "AIæœåŠ¡é…é¢ä¸è¶³",
                "message": "AIæœåŠ¡ä½¿ç”¨é‡å·²è¾¾åˆ°é™åˆ¶",
                "details": [
                    "å½“å‰æ—¶æ®µçš„APIè°ƒç”¨æ¬¡æ•°å·²ç”¨å®Œ",
                    "å¯èƒ½éœ€è¦ç­‰å¾…é…é¢é‡ç½®",
                    "æˆ–éœ€è¦å‡çº§æœåŠ¡è®¡åˆ’"
                ],
                "suggestions": [
                    "ç­‰å¾…é…é¢é‡ç½®ï¼ˆé€šå¸¸ä¸º24å°æ—¶ï¼‰",
                    "è”ç³»ç®¡ç†å‘˜å‡çº§æœåŠ¡è®¡åˆ’",
                    "æš‚æ—¶ä½¿ç”¨å…¶ä»–åˆ†æå·¥å…·",
                    "å‡å°‘åŒæ—¶è¿›è¡Œçš„åˆ†æä»»åŠ¡"
                ]
            },
            "ai_service_auth": {
                "title": "AIæœåŠ¡è®¤è¯å¤±è´¥",
                "message": "AIæœåŠ¡è®¤è¯å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ",
                "details": [
                    "APIå¯†é’¥å¯èƒ½å·²è¿‡æœŸ",
                    "æœåŠ¡é…ç½®å¯èƒ½ä¸æ­£ç¡®",
                    "è®¤è¯ä¿¡æ¯å¯èƒ½è¢«æ›´æ”¹"
                ],
                "suggestions": [
                    "è”ç³»ç³»ç»Ÿç®¡ç†å‘˜æ£€æŸ¥APIé…ç½®",
                    "ç¡®è®¤æœåŠ¡è®¢é˜…çŠ¶æ€",
                    "æ£€æŸ¥ç½‘ç»œé˜²ç«å¢™è®¾ç½®",
                    "å°è¯•é‡æ–°å¯åŠ¨æœåŠ¡"
                ]
            },
            "ai_service_error": {
                "title": "AIæœåŠ¡é”™è¯¯",
                "message": "AIåˆ†ææœåŠ¡å‡ºç°é”™è¯¯ï¼Œæ— æ³•å®Œæˆåˆ†æ",
                "details": [
                    "AIæœåŠ¡å†…éƒ¨å‡ºç°é—®é¢˜",
                    "å¯èƒ½æ˜¯ä¸´æ—¶æ€§æ•…éšœ",
                    base_error if base_error else "æœªçŸ¥AIæœåŠ¡é”™è¯¯"
                ],
                "suggestions": [
                    "ç¨åé‡æ–°å°è¯•",
                    "æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦ç¬¦åˆè¦æ±‚",
                    "å°è¯•åˆ†æå…¶ä»–æ–‡æ¡£",
                    "è”ç³»æŠ€æœ¯æ”¯æŒ"
                ]
            },
            "memory_error": {
                "title": "å†…å­˜ä¸è¶³",
                "message": "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç†æ­¤æ–‡æ¡£",
                "details": [
                    "æ–‡æ¡£è¿‡å¤§æˆ–è¿‡äºå¤æ‚",
                    "ç³»ç»Ÿå½“å‰è´Ÿè½½è¾ƒé«˜",
                    "å†…å­˜èµ„æºä¸´æ—¶ä¸è¶³"
                ],
                "suggestions": [
                    "ç¨åé‡è¯•",
                    "å°è¯•åˆ†æè¾ƒå°çš„æ–‡æ¡£",
                    "åˆ†æ‰¹å¤„ç†å¤§å‹æ–‡æ¡£",
                    "è”ç³»ç®¡ç†å‘˜ä¼˜åŒ–ç³»ç»Ÿèµ„æº"
                ]
            },
            "timeout_error": {
                "title": "å¤„ç†è¶…æ—¶",
                "message": "æ–‡æ¡£å¤„ç†è¶…æ—¶ï¼Œåˆ†ææœªèƒ½å®Œæˆ",
                "details": [
                    "æ–‡æ¡£å¤„ç†æ—¶é—´è¶…è¿‡ç³»ç»Ÿé™åˆ¶",
                    "æ–‡æ¡£å¯èƒ½è¿‡äºå¤æ‚",
                    "ç³»ç»Ÿå½“å‰å¤„ç†è´Ÿè½½è¾ƒé«˜"
                ],
                "suggestions": [
                    "ç¨åé‡è¯•",
                    "ç®€åŒ–æ–‡æ¡£å†…å®¹",
                    "åˆ†æ®µå¤„ç†å¤æ‚æ–‡æ¡£",
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§"
                ]
            },
            "unknown_error": {
                "title": "æœªçŸ¥é”™è¯¯",
                "message": f"å¤„ç†æ–‡æ¡£ '{filename}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯",
                "details": [
                    "ç³»ç»Ÿé‡åˆ°äº†é¢„æœŸä¹‹å¤–çš„é—®é¢˜",
                    base_error if base_error else "é”™è¯¯è¯¦æƒ…ä¸æ˜",
                    "å¯èƒ½æ˜¯ç³»ç»Ÿæˆ–æ–‡æ¡£çš„ç‰¹æ®Šæƒ…å†µ"
                ],
                "suggestions": [
                    "é‡æ–°å°è¯•ä¸Šä¼ å’Œåˆ†æ",
                    "æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ­£å¸¸",
                    "å°è¯•å…¶ä»–æ ¼å¼çš„æ–‡æ¡£",
                    "è”ç³»æŠ€æœ¯æ”¯æŒå¹¶æä¾›æ–‡æ¡£æ ·æœ¬"
                ]
            }
        }
        
        error_info = error_messages.get(error_type, error_messages["unknown_error"])
        
        return {
            "message": error_info["message"],
            "details": error_info["details"],
            "suggestions": error_info["suggestions"],
            "title": error_info["title"]
        }
    
    def get_supported_formats(self) -> Dict[str, Any]:
        """è·å–æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ä¿¡æ¯"""
        doc_formats = self.document_processor.get_supported_formats()
        ai_available = self.ai_client.is_available()
        
        return {
            'document_formats': doc_formats,
            'ai_available': ai_available,
            'features': {
                'pdf_extraction': doc_formats['availability']['pdf'],
                'docx_extraction': doc_formats['availability']['docx'],
                'ocr_extraction': doc_formats['availability']['ocr'],
                'ai_analysis': ai_available
            }
        }
    
    def validate_analysis_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯åˆ†æç»“æœçš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
        
        Args:
            result: AIåˆ†æç»“æœ
            
        Returns:
            Dict: éªŒè¯ç»“æœå’Œå»ºè®®
        """
        validation = {
            'valid': True,
            'warnings': [],
            'suggestions': [],
            'completeness_score': 0
        }
        
        extracted_data = result.get('extracted_data', {})
        confidence_scores = result.get('confidence_scores', {})
        
        # æ£€æŸ¥åŸºç¡€ä¿¡æ¯å®Œæ•´æ€§
        basic_info = extracted_data.get('basic_info', {})
        required_fields = ['name', 'code', 'category']
        missing_fields = [field for field in required_fields if not basic_info.get(field)]
        
        if missing_fields:
            validation['warnings'].append(f"Missing basic info: {', '.join(missing_fields)}")
            validation['suggestions'].append("Please provide the missing basic product information")
        
        # æ£€æŸ¥ç½®ä¿¡åº¦
        overall_confidence = confidence_scores.get('overall', 0)
        if overall_confidence < 0.5:
            validation['warnings'].append("Low overall confidence in analysis results")
            validation['suggestions'].append("Consider reviewing and manually verifying the extracted information")
        
        # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        completeness_factors = [
            1 if basic_info.get('name') else 0,
            1 if basic_info.get('code') else 0,
            1 if basic_info.get('category') else 0,
            1 if basic_info.get('description') else 0,
            1 if extracted_data.get('specifications') else 0,
            1 if extracted_data.get('features') else 0
        ]
        
        validation['completeness_score'] = sum(completeness_factors) / len(completeness_factors)
        
        # åŸºäºå®Œæ•´æ€§ç»™å‡ºå»ºè®®
        if validation['completeness_score'] < 0.6:
            validation['suggestions'].append("Consider providing additional product documentation for better analysis")
        
        return validation
    
    def generate_analysis_summary(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æç»“æœæ‘˜è¦"""
        if not result.get('success'):
            return f"Analysis failed: {result.get('error', 'Unknown error')}"
        
        extracted_data = result.get('extracted_data', {})
        basic_info = extracted_data.get('basic_info', {})
        confidence = result.get('confidence_scores', {}).get('overall', 0)
        
        summary_parts = []
        
        # äº§å“åç§°
        if basic_info.get('name'):
            summary_parts.append(f"Product: {basic_info['name']}")
        
        # å‹å·
        if basic_info.get('code'):
            summary_parts.append(f"Model: {basic_info['code']}")
        
        # åˆ†ç±»
        if basic_info.get('category'):
            summary_parts.append(f"Category: {basic_info['category']}")
        
        # è§„æ ¼æ•°é‡
        specs_count = len(extracted_data.get('specifications', {}))
        if specs_count > 0:
            summary_parts.append(f"Specifications: {specs_count} items")
        
        # ç‰¹æ€§æ•°é‡
        features_count = len(extracted_data.get('features', []))
        if features_count > 0:
            summary_parts.append(f"Features: {features_count} items")
        
        # ç½®ä¿¡åº¦
        summary_parts.append(f"Confidence: {confidence:.1%}")
        
        return " | ".join(summary_parts) if summary_parts else "Analysis completed with limited information"
    
    def learn_from_user_modifications(self, analysis_record_id: int, 
                                    original_data: Dict[str, Any], 
                                    final_data: Dict[str, Any],
                                    user_modifications: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»ç”¨æˆ·ä¿®æ­£ä¸­å­¦ä¹ 
        
        Args:
            analysis_record_id: åˆ†æè®°å½•ID
            original_data: åŸå§‹AIæå–æ•°æ®
            final_data: æœ€ç»ˆç¡®è®¤æ•°æ®
            user_modifications: ç”¨æˆ·ä¿®æ­£è®°å½•
            
        Returns:
            Dict: å­¦ä¹ ç»“æœ
        """
        try:
            learning_result = self.learning_engine.learn_from_modifications(
                analysis_record_id=analysis_record_id,
                original_data=original_data,
                final_data=final_data,
                user_modifications=user_modifications
            )
            
            logger.info(f"Learning completed for record {analysis_record_id}: "
                       f"{learning_result.get('patterns_identified', 0)} patterns")
            
            return learning_result
            
        except Exception as e:
            logger.error(f"Learning from modifications failed: {str(e)}")
            return {
                'patterns_identified': 0,
                'error': str(e)
            }
    
    def get_personalized_analysis_hints(self, user_id: int, document_type: str, 
                                      extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–ä¸ªæ€§åŒ–åˆ†ææç¤º
        
        Args:
            user_id: ç”¨æˆ·ID
            document_type: æ–‡æ¡£ç±»å‹
            extracted_data: æå–çš„æ•°æ®
            
        Returns:
            Dict: ä¸ªæ€§åŒ–æç¤º
        """
        try:
            hints = self.learning_engine.get_personalized_hints(
                user_id=user_id,
                document_type=document_type,
                extracted_data=extracted_data
            )
            
            return hints
            
        except Exception as e:
            logger.error(f"Failed to get personalized hints: {str(e)}")
            return {
                'personalized_hints': [],
                'predicted_modifications': {},
                'error': str(e)
            }
    
    def optimize_analysis_for_document_type(self, document_type: str, 
                                          category: str = None) -> Dict[str, Any]:
        """
        ä¸ºç‰¹å®šæ–‡æ¡£ç±»å‹ä¼˜åŒ–åˆ†æ
        
        Args:
            document_type: æ–‡æ¡£ç±»å‹
            category: äº§å“åˆ†ç±»
            
        Returns:
            Dict: ä¼˜åŒ–é…ç½®
        """
        try:
            optimization = self.learning_engine.optimize_extraction_prompt(
                document_type=document_type,
                category=category
            )
            
            return optimization
            
        except Exception as e:
            logger.error(f"Failed to optimize analysis: {str(e)}")
            return {
                'prompt_enhancements': {},
                'common_errors': [],
                'error': str(e)
            }
    
    def get_learning_statistics(self, days: int = 30) -> Dict[str, Any]:
        """
        è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            days: ç»Ÿè®¡å¤©æ•°
            
        Returns:
            Dict: å­¦ä¹ ç»Ÿè®¡
        """
        try:
            stats = self.learning_engine.get_learning_statistics(days=days)
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get learning statistics: {str(e)}")
            return {
                'total_modifications': 0,
                'avg_modification_rate': 0.0,
                'error': str(e)
            }