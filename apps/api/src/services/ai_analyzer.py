# -*- coding: utf-8 -*-
"""
AIåˆ†ææœåŠ¡
æ•´åˆæ–‡æ¡£å¤„ç†å’ŒOpenAIåˆ†æï¼Œæä¾›å®Œæ•´çš„AIäº§å“åˆ†æåŠŸèƒ½
"""
import logging
import time
import json
from typing import Dict, Any, Optional, Tuple, List
from werkzeug.datastructures import FileStorage
from datetime import datetime

from .document_processor import DocumentProcessor
from .zhipuai_client import ZhipuAIClient
from .learning_engine import LearningEngine
from .table_parser import TableParser
from .confidence_scorer import ConfidenceScorer

logger = logging.getLogger(__name__)

class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨ - ä¸“é—¨å¤„ç†AIåˆ†æç»“æœçš„è´¨é‡æ§åˆ¶"""
    
    def __init__(self):
        # æ ¼å¼å™ªå£°æ£€æµ‹æ¨¡å¼
        self.noise_patterns = [
            r'PAGE\s+\d+',          # PAGE 7 æ ¼å¼
            r'HYPERLINK',           # HYPERLINK æ ‡è®°
            r'EMBED',               # EMBED æ ‡è®°
            r'^[A-Z]\s+[A-Z]\s+[A-Z][A-Z]?\s+[A-Z]\s+[A-Z]$',  # A A AB X B ç±»å‹
            r'^[A-Z][a-z]?\s+[a-z]\s+[a-z]\s+[a-z]\s+[a-z]$',  # Ca a a a b ç±»å‹
            r'^[a-z]+\s+\d+\s+[A-Z]+.*$',  # h 9 HYPERLINK ç±»å‹
            r'^\s*[ï½œ\|]\s*$',      # å•ç‹¬çš„ç®¡é“ç¬¦
            r'^[\|\s\-\+\=]{3,}$',  # è¡¨æ ¼è¾¹æ¡†çº¿
        ]
        
        # æœ‰æ•ˆæŠ€æœ¯å‚æ•°æ¨¡å¼
        self.valid_tech_patterns = [
            r'\d+[VvAaWwHhâ„ƒâ„‰%]',    # åŒ…å«æŠ€æœ¯å•ä½
            r'\d+\s*[-~Â±]\s*\d+',   # æ•°å€¼èŒƒå›´
            r'\d+\s*[:/]\s*\d+',    # æ¯”å€¼æ ¼å¼
            r'(?:ç”µ|å‹|æµ|åŠŸ|ç‡|é¢‘|æ¸©|åº¦|ç²¾|é‡)',  # ä¸­æ–‡æŠ€æœ¯å…³é”®è¯
            r'(?:volt|amp|watt|freq|temp|test|spec)',  # è‹±æ–‡æŠ€æœ¯è¯
        ]
    
    def validate_extracted_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯å’Œæ¸…æ´æå–çš„æ•°æ®
        
        Args:
            data: AIåˆ†ææå–çš„åŸå§‹æ•°æ®
            
        Returns:
            Dict: åŒ…å«éªŒè¯ç»“æœå’Œæ¸…æ´åçš„æ•°æ®
        """
        validation_report = {
            'original_specs_count': 0,
            'noise_removed_count': 0,
            'invalid_removed_count': 0,
            'final_specs_count': 0,
            'quality_issues': [],
            'confidence_adjustments': {}
        }
        
        if not isinstance(data, dict):
            return {
                'cleaned_data': data,
                'validation_report': validation_report,
                'data_quality_score': 0.0
            }
        
        cleaned_data = data.copy()
        
        # éªŒè¯å’Œæ¸…æ´specificationså­—æ®µ
        if 'specifications' in cleaned_data:
            cleaned_specs, spec_validation = self._clean_specifications(cleaned_data['specifications'])
            cleaned_data['specifications'] = cleaned_specs
            validation_report.update(spec_validation)
        
        # éªŒè¯basic_infoå­—æ®µ
        if 'basic_info' in cleaned_data:
            basic_validation = self._validate_basic_info(cleaned_data['basic_info'])
            validation_report['quality_issues'].extend(basic_validation['issues'])
        
        # è°ƒæ•´ç½®ä¿¡åº¦è¯„åˆ†
        if 'confidence' in cleaned_data:
            adjusted_confidence = self._adjust_confidence_scores(
                cleaned_data['confidence'], 
                validation_report
            )
            cleaned_data['confidence'] = adjusted_confidence
            validation_report['confidence_adjustments'] = adjusted_confidence
        
        # è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(validation_report)
        
        # è®°å½•è´¨é‡é—®é¢˜
        if validation_report['quality_issues']:
            logger.warning(f"æ•°æ®è´¨é‡é—®é¢˜: {len(validation_report['quality_issues'])}ä¸ª, "
                         f"è´¨é‡è¯„åˆ†: {quality_score:.2f}")
        
        return {
            'cleaned_data': cleaned_data,
            'validation_report': validation_report,
            'data_quality_score': quality_score
        }
    
    def _clean_specifications(self, specifications: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """æ¸…æ´è§„æ ¼å‚æ•°æ•°æ®"""
        import re
        
        cleaned_specs = {}
        validation_report = {
            'original_specs_count': len(specifications),
            'noise_removed_count': 0,
            'invalid_removed_count': 0,
            'final_specs_count': 0,
            'removed_specs': []
        }
        
        for spec_name, spec_data in specifications.items():
            if not spec_name or not spec_name.strip():
                continue
                
            spec_name = spec_name.strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ¼å¼å™ªå£°
            is_noise = False
            for pattern in self.noise_patterns:
                if re.search(pattern, spec_name, re.IGNORECASE):
                    validation_report['noise_removed_count'] += 1
                    validation_report['removed_specs'].append({
                        'name': spec_name,
                        'reason': 'format_noise',
                        'pattern': pattern
                    })
                    is_noise = True
                    break
            
            if is_noise:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæŠ€æœ¯å‚æ•°
            is_valid_tech = False
            spec_value = ""
            
            if isinstance(spec_data, dict):
                spec_value = str(spec_data.get('value', ''))
            else:
                spec_value = str(spec_data)
            
            combined_text = f"{spec_name} {spec_value}"
            
            # æ£€æŸ¥æŠ€æœ¯æœ‰æ•ˆæ€§
            for pattern in self.valid_tech_patterns:
                if re.search(pattern, combined_text, re.IGNORECASE):
                    is_valid_tech = True
                    break
            
            # å¦‚æœåŒ…å«æ˜æ˜¾çš„æŠ€æœ¯ä¿¡æ¯ï¼Œåˆ™è®¤ä¸ºæœ‰æ•ˆ
            if (len(spec_name) > 1 and 
                (re.search(r'\d', combined_text) or  # åŒ…å«æ•°å­—
                 re.search(r'[ç”µå‹æµåŠŸç‡é¢‘æ¸©åº¦ç²¾é‡]', combined_text) or  # åŒ…å«æŠ€æœ¯å…³é”®è¯
                 re.search(r'[VvAaWwHhâ„ƒâ„‰%]', combined_text))):  # åŒ…å«æŠ€æœ¯å•ä½
                is_valid_tech = True
            
            if is_valid_tech:
                cleaned_specs[spec_name] = spec_data
            else:
                validation_report['invalid_removed_count'] += 1
                validation_report['removed_specs'].append({
                    'name': spec_name,
                    'reason': 'not_technical',
                    'value': spec_value
                })
        
        validation_report['final_specs_count'] = len(cleaned_specs)
        
        return cleaned_specs, validation_report
    
    def _validate_basic_info(self, basic_info: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯åŸºæœ¬ä¿¡æ¯å­—æ®µ"""
        issues = []
        
        if not basic_info.get('name'):
            issues.append("äº§å“åç§°ç¼ºå¤±")
        elif len(basic_info['name'].strip()) < 3:
            issues.append("äº§å“åç§°è¿‡çŸ­ï¼Œå¯èƒ½ä¸å‡†ç¡®")
            
        if not basic_info.get('code'):
            issues.append("äº§å“å‹å·ç¼ºå¤±")
            
        if not basic_info.get('category'):
            issues.append("äº§å“åˆ†ç±»ç¼ºå¤±")
        
        return {'issues': issues}
    
    def _adjust_confidence_scores(self, confidence: Dict[str, Any], validation_report: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®éªŒè¯ç»“æœè°ƒæ•´ç½®ä¿¡åº¦è¯„åˆ†"""
        adjusted_confidence = confidence.copy()
        
        # å¦‚æœå‘ç°å¤§é‡æ ¼å¼å™ªå£°ï¼Œé™ä½specificationsç½®ä¿¡åº¦
        noise_ratio = validation_report.get('noise_removed_count', 0) / max(validation_report.get('original_specs_count', 1), 1)
        if noise_ratio > 0.3:  # å¦‚æœè¶…è¿‡30%æ˜¯å™ªå£°
            if 'specifications' in adjusted_confidence:
                original_spec_confidence = adjusted_confidence['specifications']
                adjusted_confidence['specifications'] = original_spec_confidence * (1 - noise_ratio * 0.5)
                logger.warning(f"ç”±äºå‘ç°{noise_ratio:.1%}çš„æ ¼å¼å™ªå£°ï¼Œè§„æ ¼å‚æ•°ç½®ä¿¡åº¦ä»{original_spec_confidence:.3f}è°ƒæ•´ä¸º{adjusted_confidence['specifications']:.3f}")
        
        # é‡æ–°è®¡ç®—overallç½®ä¿¡åº¦
        if 'basic_info' in adjusted_confidence and 'specifications' in adjusted_confidence:
            adjusted_confidence['overall'] = (
                adjusted_confidence['basic_info'] * 0.3 + 
                adjusted_confidence['specifications'] * 0.7
            )
        
        return adjusted_confidence
    
    def _calculate_quality_score(self, validation_report: Dict[str, Any]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼‰"""
        score = 1.0
        
        # æ ¹æ®å™ªå£°æ¯”ä¾‹æ‰£åˆ†
        original_count = validation_report.get('original_specs_count', 1)
        noise_count = validation_report.get('noise_removed_count', 0)
        invalid_count = validation_report.get('invalid_removed_count', 0)
        
        noise_ratio = noise_count / max(original_count, 1)
        invalid_ratio = invalid_count / max(original_count, 1)
        
        # å™ªå£°æ•°æ®æ‰£åˆ†
        score -= noise_ratio * 0.5  # æœ€å¤šæ‰£0.5åˆ†
        
        # æ— æ•ˆæ•°æ®æ‰£åˆ†
        score -= invalid_ratio * 0.3  # æœ€å¤šæ‰£0.3åˆ†
        
        # è´¨é‡é—®é¢˜æ‰£åˆ†
        issues_count = len(validation_report.get('quality_issues', []))
        score -= issues_count * 0.05  # æ¯ä¸ªé—®é¢˜æ‰£0.05åˆ†
        
        return max(0.0, min(1.0, score))

class AnalysisMonitor:
    """åˆ†æè¿‡ç¨‹ç›‘æ§å™¨ - æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯å’Œæ€§èƒ½ç»Ÿè®¡"""
    
    def __init__(self):
        self.start_time = None
        self.stages = {}
        self.metrics = {
            'document_info': {},
            'text_extraction': {},
            'table_parsing': {},
            'ai_analysis': {},
            'quality_assessment': {},
            'overall': {}
        }
    
    def start_analysis(self, filename: str, file_size: int):
        """å¼€å§‹åˆ†æç›‘æ§"""
        self.start_time = time.time()
        self.metrics['document_info'] = {
            'filename': filename,
            'file_size': file_size,
            'start_time': datetime.now().isoformat()
        }
        logger.info(f"ğŸ“Š å¼€å§‹åˆ†æç›‘æ§: {filename} ({file_size} bytes)")
    
    def stage_start(self, stage_name: str):
        """é˜¶æ®µå¼€å§‹"""
        self.stages[stage_name] = {'start': time.time()}
        logger.info(f"ğŸ”„ é˜¶æ®µå¼€å§‹: {stage_name}")
    
    def stage_end(self, stage_name: str, **kwargs):
        """é˜¶æ®µç»“æŸ"""
        if stage_name in self.stages:
            duration = time.time() - self.stages[stage_name]['start']
            self.stages[stage_name].update({
                'duration': duration,
                'end': time.time(),
                **kwargs
            })
            logger.info(f"âœ… é˜¶æ®µå®Œæˆ: {stage_name} ({duration:.2f}s)")
    
    def record_metrics(self, category: str, data: Dict[str, Any]):
        """è®°å½•è¯¦ç»†æŒ‡æ ‡"""
        self.metrics[category].update(data)
        logger.debug(f"ğŸ“Š æŒ‡æ ‡è®°å½• [{category}]: {data}")
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦"""
        total_duration = time.time() - self.start_time if self.start_time else 0
        
        return {
            'total_duration': round(total_duration, 2),
            'stages': {name: stage.get('duration', 0) for name, stage in self.stages.items()},
            'metrics': self.metrics,
            'completion_time': datetime.now().isoformat()
        }

class AIAnalyzer:
    """AIäº§å“åˆ†æå™¨ - é›†æˆç›‘æ§å’Œè°ƒè¯•åŠŸèƒ½"""
    
    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.ai_client = ZhipuAIClient()
        self.learning_engine = LearningEngine()
        self.table_parser = TableParser()
        self.confidence_scorer = ConfidenceScorer()
        self.quality_validator = DataQualityValidator()  # æ•°æ®è´¨é‡éªŒè¯å™¨
        self.monitor = None  # åŠ¨æ€åˆ›å»ºç›‘æ§å™¨
    
    def analyze_product_document(self, file: FileStorage, user_id: int = None) -> Dict[str, Any]:
        """
        åˆ†æäº§å“æ–‡æ¡£ï¼Œæå–äº§å“ä¿¡æ¯ - é›†æˆè¯¦ç»†ç›‘æ§å’Œè°ƒè¯•
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            user_id: ç”¨æˆ·IDï¼Œç”¨äºä¸ªæ€§åŒ–åˆ†æ
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æç»“æœï¼ŒåŒ…å«è°ƒè¯•ä¿¡æ¯
        """
        # ğŸ”§ åˆå§‹åŒ–ç›‘æ§å™¨
        self.monitor = AnalysisMonitor()
        file_size = len(file.read())
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        self.monitor.start_analysis(file.filename, file_size)
        
        try:
            # ğŸ”„ é˜¶æ®µ1: æ–‡æ¡£å¤„ç†å’Œæ–‡æœ¬æå–
            self.monitor.stage_start("document_processing")
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file.filename} ({file_size} bytes)")
            
            text_content, doc_info = self.document_processor.process_document(file)
            
            # è®°å½•æ–‡æ¡£å¤„ç†æŒ‡æ ‡
            self.monitor.record_metrics('text_extraction', {
                'text_length': len(text_content),
                'doc_format': doc_info.get('format', 'unknown'),
                'pages': doc_info.get('pages', 0),
                'encoding': doc_info.get('encoding', 'unknown')
            })
            
            if not text_content.strip():
                raise ValueError("Document contains no readable text content")
                
            self.monitor.stage_end("document_processing", 
                                 text_chars=len(text_content), 
                                 format=doc_info.get('format', 'unknown'))
            
            # ğŸ” å¢å¼ºä¹±ç æ£€æµ‹ - ä½¿ç”¨æ–‡æ¡£å¤„ç†å™¨çš„å¢å¼ºç®—æ³•
            if self.document_processor._enhanced_corruption_detection(text_content, doc_info.get('type', '')):
                error_msg = self._generate_corruption_error_message(file.filename, doc_info)
                raise ValueError(error_msg)
            
            # ğŸ”„ é˜¶æ®µ2: ä¸ªæ€§åŒ–å­¦ä¹ å¼•æ“
            personalized_hints = {}
            if user_id:
                self.monitor.stage_start("personalization")
                personalized_hints = self.learning_engine.get_personalized_hints(
                    user_id=user_id,
                    document_type=doc_info.get('type', 'unknown'),
                    extracted_data={}
                )
                self.monitor.stage_end("personalization", hints_count=len(personalized_hints))
            
            # ğŸ”„ é˜¶æ®µ3: AIæ–‡æ¡£åˆ†æï¼ˆåˆ†å±‚åˆ†æï¼‰
            self.monitor.stage_start("ai_analysis")
            logger.info(f"ğŸ¤– å¼€å§‹AIåˆ†æï¼Œæ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
            
            ai_result = self.ai_client.analyze_product_document(
                document_content=text_content,
                document_name=file.filename or "unknown"
            )
            
            # è®°å½•AIåˆ†ææŒ‡æ ‡
            ai_specs_count = len(ai_result.get('specifications', {}))
            ai_confidence = ai_result.get('confidence', {}).get('overall', 0)
            self.monitor.record_metrics('ai_analysis', {
                'specifications_extracted': ai_specs_count,
                'ai_confidence': ai_confidence,
                'has_basic_info': bool(ai_result.get('basic_info', {}).get('name', ''))
            })
            self.monitor.stage_end("ai_analysis", 
                                 specs_count=ai_specs_count, 
                                 confidence=ai_confidence)
            
            # ğŸ”„ é˜¶æ®µ4: è¡¨æ ¼è§£æå¢å¼º
            self.monitor.stage_start("table_parsing")
            logger.info(f"ğŸ“Š å¼€å§‹è¡¨æ ¼è§£æå¢å¼º")
            
            enhanced_result = self.table_parser.enhance_extraction_with_tables(ai_result, text_content)
            
            # è®°å½•è¡¨æ ¼è§£ææŒ‡æ ‡
            enhanced_specs_count = len(enhanced_result.get('specifications', {}))
            table_info = enhanced_result.get('table_parsing', {})
            self.monitor.record_metrics('table_parsing', {
                'tables_found': table_info.get('tables_found', 0),
                'parsing_confidence': table_info.get('parsing_confidence', 0),
                'specs_after_enhancement': enhanced_specs_count,
                'enhancement_gain': enhanced_specs_count - ai_specs_count
            })
            self.monitor.stage_end("table_parsing", 
                                 tables_found=table_info.get('tables_found', 0),
                                 final_specs=enhanced_specs_count)
            
            # ğŸ”„ é˜¶æ®µ5: æ•°æ®è´¨é‡éªŒè¯å’Œæ¸…æ´
            self.monitor.stage_start("data_quality_validation")
            logger.info("ğŸ” å¼€å§‹æ•°æ®è´¨é‡éªŒè¯å’Œæ¸…æ´...")
            
            quality_validation = self.quality_validator.validate_extracted_data(enhanced_result)
            enhanced_result = quality_validation['cleaned_data']
            validation_report = quality_validation['validation_report']
            data_quality_score = quality_validation['data_quality_score']
            
            # è®°å½•è´¨é‡éªŒè¯ç»“æœ
            logger.info(f"ğŸ“Š æ•°æ®è´¨é‡éªŒè¯å®Œæˆ - è´¨é‡è¯„åˆ†: {data_quality_score:.2f}")
            if validation_report['noise_removed_count'] > 0:
                logger.info(f"ğŸ§¹ æ¸…é™¤æ ¼å¼å™ªå£°: {validation_report['noise_removed_count']}é¡¹")
            if validation_report['invalid_removed_count'] > 0:
                logger.info(f"ğŸ—‘ï¸ æ¸…é™¤æ— æ•ˆå‚æ•°: {validation_report['invalid_removed_count']}é¡¹")
            logger.info(f"âœ… æœ€ç»ˆæœ‰æ•ˆè§„æ ¼å‚æ•°: {validation_report['final_specs_count']}é¡¹")
            
            self.monitor.stage_end("data_quality_validation", 
                                 quality_score=data_quality_score,
                                 noise_removed=validation_report['noise_removed_count'],
                                 invalid_removed=validation_report['invalid_removed_count'])
            
            # ğŸ”„ é˜¶æ®µ6: è´¨é‡è¯„ä¼°å’Œç½®ä¿¡åº¦è®¡ç®—
            self.monitor.stage_start("quality_assessment")
            confidence_scores = self.confidence_scorer.calculate_comprehensive_confidence(
                extracted_data=enhanced_result,
                document_info=doc_info,
                historical_context=personalized_hints.get('pattern_context') if user_id else None
            )
            
            # å¦‚æœæ•°æ®è´¨é‡éªŒè¯å™¨è°ƒæ•´äº†ç½®ä¿¡åº¦ï¼Œä½¿ç”¨è°ƒæ•´åçš„å€¼
            if 'confidence_adjustments' in validation_report and validation_report['confidence_adjustments']:
                confidence_scores = validation_report['confidence_adjustments']
            
            self.monitor.stage_end("quality_assessment", confidence=confidence_scores.get('overall', 0))
            
            # ğŸ”„ é˜¶æ®µ7: æ•°æ®åå¤„ç†å’Œä¿®å¤
            self.monitor.stage_start("data_postprocessing")
            
            # æ™ºèƒ½ä¿®å¤äº§å“åç§°ï¼ˆå¦‚æœè§„æ ¼æå–æˆåŠŸä½†åç§°ä¸ºç©ºï¼‰
            current_name = enhanced_result.get('basic_info', {}).get('name', '')
            logger.info(f"ğŸ” æ£€æŸ¥äº§å“åç§°ä¿®å¤ - å½“å‰åç§°: '{current_name}', æœ‰è§„æ ¼å‚æ•°: {bool(enhanced_result.get('specifications'))}")
            
            if not current_name.strip() and enhanced_result.get('specifications'):
                logger.info(f"ğŸ”§ å¼€å§‹ä¿®å¤ç¼ºå¤±çš„äº§å“åç§°ï¼Œæ–‡ä»¶å: {file.filename}")
                enhanced_result = self._fix_missing_product_name(enhanced_result, file.filename or "unknown")
                new_name = enhanced_result.get('basic_info', {}).get('name', '')
                logger.info(f"âœ… äº§å“åç§°ä¿®å¤å®Œæˆ: '{new_name}'")
            
            # æœ€ç»ˆæ•°æ®æ¸…ç† - ç¡®ä¿æ‰€æœ‰è§„æ ¼å‚æ•°éƒ½æ˜¯æœ‰æ•ˆçš„
            final_specs_count = 0
            if enhanced_result.get('specifications'):
                cleaned_specs = self.table_parser._clean_specification_data(enhanced_result['specifications'])
                enhanced_result['specifications'] = cleaned_specs
                final_specs_count = len(cleaned_specs)
                logger.info(f"æœ€ç»ˆè§„æ ¼å‚æ•°æ¸…ç†å®Œæˆï¼Œä¿ç•™ {final_specs_count} é¡¹æœ‰æ•ˆå‚æ•°")
            
            self.monitor.stage_end("data_postprocessing", final_specs_count=final_specs_count)
            
            # ğŸ”„ å®Œæˆåˆ†æï¼Œè·å–ç›‘æ§æ€»ç»“
            monitor_summary = self.monitor.get_summary()
            
            # è®°å½•æœ€ç»ˆè´¨é‡æŒ‡æ ‡
            final_basic_info = enhanced_result.get('basic_info', {})
            final_specs = enhanced_result.get('specifications', {})
            
            self.monitor.record_metrics('overall', {
                'final_specs_count': len(final_specs),
                'has_product_name': bool(final_basic_info.get('name', '').strip()),
                'has_product_code': bool(final_basic_info.get('code', '').strip()),
                'has_category': bool(final_basic_info.get('category', '').strip()),
                'overall_confidence': confidence_scores.get('overall', 0)
            })
            
            # æ‰“å°è¯¦ç»†çš„åˆ†ææ€»ç»“
            logger.info(f"ğŸ“Š === åˆ†ææ€»ç»“ ===")
            logger.info(f"ğŸ• æ€»è€—æ—¶: {monitor_summary['total_duration']}ç§’")
            for stage, duration in monitor_summary['stages'].items():
                logger.info(f"   {stage}: {duration:.2f}s")
            logger.info(f"ğŸ“„ æ–‡æ¡£ä¿¡æ¯: {file.filename} ({file_size} bytes)")
            logger.info(f"ğŸ¤– AIæå–: {len(ai_result.get('specifications', {}))} é¡¹è§„æ ¼")
            logger.info(f"ğŸ“Š è¡¨æ ¼å¢å¼º: +{enhanced_specs_count - ai_specs_count} é¡¹è§„æ ¼")
            logger.info(f"ğŸ¯ æœ€ç»ˆç»“æœ: {len(final_specs)} é¡¹è§„æ ¼ï¼Œç½®ä¿¡åº¦ {confidence_scores.get('overall', 0):.3f}")
            logger.info(f"================")
            
            # æ„å»ºå®Œæ•´ç»“æœï¼ˆåŒ…å«è°ƒè¯•ä¿¡æ¯å’Œè´¨é‡éªŒè¯ï¼‰
            result = {
                'success': True,
                'document_info': {
                    **doc_info,
                    'analysis_duration': monitor_summary['total_duration']
                },
                'extracted_data': enhanced_result,
                'confidence_scores': confidence_scores,
                'personalized_hints': personalized_hints.get('personalized_hints', []) if user_id else [],
                'predicted_modifications': personalized_hints.get('predicted_modifications', {}) if user_id else {},
                'text_preview': text_content[:500] + "..." if len(text_content) > 500 else text_content,
                'analysis_timestamp': int(time.time()),
                
                # ğŸ†• æ•°æ®è´¨é‡ç›¸å…³ä¿¡æ¯
                'data_quality_score': data_quality_score,
                'validation_report': validation_report,
                
                # ğŸ†• è¯¦ç»†çš„è°ƒè¯•å’Œç›‘æ§ä¿¡æ¯
                'debug_info': {
                    'monitor_summary': monitor_summary,
                    'stage_breakdown': monitor_summary['stages'],
                    'performance_metrics': monitor_summary['metrics'],
                    'processing_pipeline': [
                        f"æ–‡æ¡£å¤„ç†: {doc_info.get('format', 'unknown')} -> {len(text_content)} å­—ç¬¦",
                        f"AIåˆ†æ: {ai_specs_count} é¡¹è§„æ ¼ (ç½®ä¿¡åº¦ {ai_confidence:.3f})",
                        f"è¡¨æ ¼å¢å¼º: +{enhanced_specs_count - ai_specs_count} é¡¹è§„æ ¼",
                        f"è´¨é‡éªŒè¯: ç§»é™¤ {validation_report.get('noise_removed_count', 0)} å™ªå£°, {validation_report.get('invalid_removed_count', 0)} æ— æ•ˆæ•°æ®",
                        f"è´¨é‡è¯„ä¼°: ç½®ä¿¡åº¦ {confidence_scores.get('overall', 0):.3f}, è´¨é‡è¯„åˆ† {data_quality_score:.3f}",
                        f"æ•°æ®åå¤„ç†: æœ€ç»ˆ {final_specs_count} é¡¹è§„æ ¼"
                    ]
                }
            }
            
            logger.info(f"âœ… æ–‡æ¡£åˆ†ææˆåŠŸå®Œæˆ - è€—æ—¶: {monitor_summary['total_duration']}s, ç½®ä¿¡åº¦: {confidence_scores.get('overall', 'N/A')}")
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ AIåˆ†æå¤±è´¥ - æ–‡ä»¶: {file.filename}, é”™è¯¯: {error_msg}")
            
            # è·å–å¤±è´¥æ—¶çš„ç›‘æ§ä¿¡æ¯
            if self.monitor:
                error_monitor_summary = self.monitor.get_summary()
                logger.error(f"ğŸ’” å¤±è´¥é˜¶æ®µåˆ†æ: {error_monitor_summary}")
            
            # ğŸ”§ å¢å¼ºé”™è¯¯åˆ†ç±»å’Œå¤„ç†
            error_type = self._classify_error_type(e, error_msg)
            detailed_error = self._generate_detailed_error_message(e, error_type, file.filename, doc_info if 'doc_info' in locals() else {})
            
            return {
                'success': False,
                'error': detailed_error['message'],
                'error_type': error_type,
                'error_details': detailed_error['details'],
                'suggestions': detailed_error['suggestions'],
                'document_info': {
                    'filename': file.filename,
                    'analysis_duration': error_monitor_summary['total_duration'] if self.monitor else 0,
                    'file_type': getattr(file, 'mimetype', 'unknown')
                },
                # ğŸ†• é”™è¯¯è°ƒè¯•ä¿¡æ¯
                'debug_info': {
                    'error_monitor_summary': error_monitor_summary if self.monitor else {},
                    'failed_stage': max(error_monitor_summary['stages'].keys()) if self.monitor and error_monitor_summary['stages'] else 'unknown',
                    'error_timestamp': datetime.now().isoformat()
                }
            }
    
    def _fix_missing_product_name(self, extracted_data: Dict[str, Any], filename: str) -> Dict[str, Any]:
        """æ™ºèƒ½ä¿®å¤ç¼ºå¤±çš„äº§å“åç§°"""
        try:
            basic_info = extracted_data.get('basic_info', {})
            specifications = extracted_data.get('specifications', {})
            
            # ä»æ–‡ä»¶åæ¨æ–­äº§å“åç§°
            if 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª' in filename or 'ç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª' in filename:
                basic_info['name'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª'
                basic_info['category'] = 'ç»§ç”µä¿æŠ¤æµ‹è¯•è®¾å¤‡'
                basic_info['description'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ªï¼Œç”¨äºç”µåŠ›ç³»ç»Ÿç»§ç”µä¿æŠ¤è£…ç½®çš„å…¨é¢æµ‹è¯•å’Œæ ¡éªŒ'
                
            elif 'å˜å‹å™¨' in filename:
                basic_info['name'] = 'ç”µåŠ›å˜å‹å™¨'
                basic_info['category'] = 'å˜å‹å™¨è®¾å¤‡'
                
            elif 'å¼€å…³' in filename:
                basic_info['name'] = 'ç”µåŠ›å¼€å…³è®¾å¤‡'
                basic_info['category'] = 'å¼€å…³è®¾å¤‡'
                
            else:
                # ä»è§„æ ¼å‚æ•°æ¨æ–­äº§å“ç±»å‹
                if any('ç›¸' in str(key) for key in specifications.keys()):
                    if '6' in str(specifications) or 'å…­' in str(specifications):
                        basic_info['name'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ª'
                        basic_info['category'] = 'ç»§ç”µä¿æŠ¤æµ‹è¯•è®¾å¤‡'
                        basic_info['description'] = 'å…­ç›¸å¾®æœºç»§ç”µä¿æŠ¤æµ‹è¯•ä»ªï¼Œç”¨äºç”µåŠ›ç³»ç»Ÿç»§ç”µä¿æŠ¤è£…ç½®çš„æµ‹è¯•'
                    else:
                        basic_info['name'] = 'ç”µåŠ›æµ‹è¯•è®¾å¤‡'
                        basic_info['category'] = 'ç”µåŠ›æµ‹è¯•è®¾å¤‡'
                        
                elif any('å˜å‹å™¨' in str(key) for key in specifications.keys()):
                    basic_info['name'] = 'ç”µåŠ›å˜å‹å™¨'
                    basic_info['category'] = 'å˜å‹å™¨è®¾å¤‡'
                    
                else:
                    # é€šç”¨ç”µæ°”è®¾å¤‡
                    basic_info['name'] = 'ç”µåŠ›è®¾å¤‡'
                    basic_info['category'] = 'ç”µåŠ›è®¾å¤‡'
            
            # æ›´æ–°ç½®ä¿¡åº¦
            confidence = extracted_data.get('confidence', {})
            if basic_info.get('name'):
                confidence['basic_info'] = max(confidence.get('basic_info', 0), 0.8)
                confidence['overall'] = max(confidence.get('overall', 0), 
                                          (confidence.get('basic_info', 0.8) + confidence.get('specifications', 0.7)) / 2)
            
            extracted_data['basic_info'] = basic_info
            extracted_data['confidence'] = confidence
            
            logger.info(f"æ™ºèƒ½ä¿®å¤äº§å“åç§°: {basic_info.get('name', 'N/A')}")
            
            return extracted_data
            
        except Exception as e:
            logger.error(f"ä¿®å¤äº§å“åç§°å¤±è´¥: {str(e)}")
            return extracted_data
    
    def _is_text_corrupted(self, text: str, file_extension: str = '') -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸¥é‡ä¹±ç ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ"""
        if not text or len(text.strip()) < 20:  # ğŸ”§ è¿›ä¸€æ­¥é™ä½æœ€å°é•¿åº¦è¦æ±‚ï¼ŒOCRæ–‡æœ¬å¯èƒ½å¾ˆçŸ­
            return True
        
        # ğŸ–¼ï¸ ç‰¹æ®Šå¤„ç†å›¾ç‰‡æ–‡ä»¶çš„OCRæ–‡æœ¬
        is_image_ocr = file_extension.lower() in ['.png', '.jpg', '.jpeg', '.gif', '.bmp']
        
        text_sample = text[:3000]  # å–å‰3000å­—ç¬¦è¿›è¡Œæ£€æµ‹
        
        # ğŸ” ä¹±ç ç‰¹å¾æ£€æµ‹
        import re
        
        # 1. æ£€æµ‹æ˜æ˜¾çš„ç¼–ç é”™è¯¯å­—ç¬¦ - å¯¹å›¾ç‰‡OCRæ›´å®½æ¾
        corruption_chars = sum(1 for c in text_sample if c in 'æ€é‡¨å¨é†¢è— ä¿¹ç‰æ…µæ¥´æ¹¯ç•±ç‘¡æ½©åæ½—æ‘²æ½„å€ç‘©æ•¬æ¥ç‰£ç¯æ™¯ç……æ…µæ¥´æ¹¯')
        corruption_threshold = 0.5 if is_image_ocr else 0.3  # ğŸ–¼ï¸ å›¾ç‰‡OCRå…è®¸æ›´å¤šé”™è¯¯å­—ç¬¦
        if corruption_chars / len(text_sample) > corruption_threshold:
            return True
            
        # 2. æ£€æµ‹é‡å¤æ¨¡å¼è¿‡å¤š - å¯¹å›¾ç‰‡OCRæ›´å®½æ¾
        repeated_patterns = re.findall(r'(.{2,4})\1{5,}', text_sample)
        repeat_threshold = 25 if is_image_ocr else 15  # ğŸ–¼ï¸ å›¾ç‰‡OCRå…è®¸æ›´å¤šé‡å¤æ¨¡å¼
        if len(repeated_patterns) > repeat_threshold:
            return True
            
        # 3. æ£€æµ‹å¯è¯»æ€§ - å¯è¯»çš„ä¸­æ–‡æˆ–è‹±æ–‡å†…å®¹å¤ªå°‘
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text_sample))
        english_words = len(re.findall(r'\b[a-zA-Z]{2,}\b', text_sample))  # ğŸ”§ é™ä½è‹±æ–‡å•è¯é•¿åº¦è¦æ±‚
        digits = len(re.findall(r'\d', text_sample))
        symbols = len(re.findall(r'[A-Za-z]\d+|\d+[A-Za-z]', text_sample))  # ğŸ”§ æ·»åŠ å­—æ¯æ•°å­—ç»„åˆæ£€æµ‹
        
        # è®¡ç®—å¯è¯»å†…å®¹æ¯”ä¾‹ - å¯¹å›¾ç‰‡OCRæ›´å®½æ¾
        readable_content = chinese_chars + english_words * 2 + digits * 0.8 + symbols * 1.5  # ğŸ–¼ï¸ è°ƒæ•´æƒé‡ï¼Œé‡è§†æ•°å­—å’Œç¬¦å·
        readable_ratio = readable_content / len(text_sample)
        
        # ğŸ”§ é’ˆå¯¹ä¸åŒæ–‡ä»¶ç±»å‹çš„ç‰¹æ®Šæ£€æµ‹
        # æ£€æµ‹æ˜¯å¦æœ‰æŠ€æœ¯å…³é”®è¯æˆ–æ•°å­—+å•ä½çš„ç»„åˆ
        has_technical_content = bool(re.search(
            r'(ç”µå‹|ç”µæµ|åŠŸç‡|é¢‘ç‡|æµ‹è¯•|ä¿æŠ¤|ç»§ç”µ|è£…ç½®|è®¾å¤‡|æ€§èƒ½|ç²¾åº¦|è§„æ ¼|å‚æ•°|å‹å·|'
            r'voltage|current|power|frequency|test|specification|model|type|parameter)',
            text_sample, re.IGNORECASE
        ))
        has_numeric_units = bool(re.search(r'\d+\s*[A-Za-z%Â°â„ƒÎ©Î©Î¼Î¼Â±]+', text_sample))
        has_product_codes = bool(re.search(r'[A-Z0-9-]{3,}', text_sample))  # ğŸ”§ æ£€æµ‹äº§å“ç¼–ç æ¨¡å¼
        
        # ğŸ–¼ï¸ å›¾ç‰‡OCRçš„ç‰¹æ®Šæ£€æµ‹ - æ›´å®½æ¾çš„åˆ¤æ–­
        if is_image_ocr:
            # å¦‚æœæœ‰ä»»ä½•æŠ€æœ¯å†…å®¹ã€æ•°å­—å•ä½æˆ–äº§å“ç¼–ç ï¼Œæå¤§é™ä½å¯è¯»æ€§è¦æ±‚
            if has_technical_content or has_numeric_units or has_product_codes:
                min_readable_ratio = 0.02  # ğŸ–¼ï¸ æä½é˜ˆå€¼ï¼Œåªè¦æœ‰ä¸€ç‚¹æŠ€æœ¯å†…å®¹å°±æ¥å—
            else:
                min_readable_ratio = 0.03  # ğŸ–¼ï¸ ç¨ä½é˜ˆå€¼
        else:
            # æ™®é€šæ–‡æ¡£çš„æ£€æµ‹
            if has_technical_content or has_numeric_units:
                min_readable_ratio = 0.05
            else:
                min_readable_ratio = 0.08
        
        if readable_ratio < min_readable_ratio:
            logger.debug(f"ğŸ“„ æ–‡æœ¬å¯è¯»æ€§æ£€æµ‹ - æ–‡ä»¶ç±»å‹: {'å›¾ç‰‡OCR' if is_image_ocr else 'æ™®é€šæ–‡æ¡£'}")
            logger.debug(f"âŒ å¯è¯»æ€§ä¸è¶³: {readable_ratio:.3f} < {min_readable_ratio} "
                        f"(ä¸­æ–‡:{chinese_chars}, è‹±æ–‡:{english_words}, æ•°å­—:{digits}, ç¬¦å·:{symbols})")
            logger.debug(f"ğŸ” æŠ€æœ¯å†…å®¹: {has_technical_content}, æ•°å­—å•ä½: {has_numeric_units}, äº§å“ç¼–ç : {has_product_codes}")
            return True
        
        logger.debug(f"âœ… æ–‡æœ¬è´¨é‡æ£€æµ‹é€šè¿‡ - å¯è¯»æ€§: {readable_ratio:.3f} >= {min_readable_ratio}")
        return False
    
    def _generate_corruption_error_message(self, filename: str, doc_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¹±ç é”™è¯¯çš„è¯¦ç»†ä¿¡æ¯"""
        file_type = doc_info.get('type', 'unknown')
        file_size = doc_info.get('size', 0)
        
        error_msg = f"æ–‡æ¡£åˆ†æå¤±è´¥ï¼šæ— æ³•ä»æ–‡ä»¶ '{filename}' ä¸­æå–å¯è¯»æ–‡æœ¬å†…å®¹\n\n"
        
        # ğŸ” å…·ä½“è¯Šæ–­ä¿¡æ¯
        error_msg += "é—®é¢˜è¯Šæ–­ï¼š\n"
        error_msg += f"â€¢ æ–‡ä»¶ç±»å‹: {file_type.upper()}\n"
        error_msg += f"â€¢ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚\n"
        error_msg += "â€¢ æ£€æµ‹åˆ°æ–‡æœ¬ç¼–ç é—®é¢˜æˆ–æ–‡ä»¶æ ¼å¼å¼‚å¸¸\n\n"
        
        # ğŸ› ï¸ è§£å†³å»ºè®®
        error_msg += "å»ºè®®çš„è§£å†³æ–¹æ¡ˆï¼š\n"
        
        if file_type == 'doc':
            error_msg += "1. å°†æ–‡ä»¶è½¬æ¢ä¸º .docx æ ¼å¼ï¼ˆæ¨èï¼‰\n"
            error_msg += "2. ä½¿ç”¨Microsoft Wordæ‰“å¼€æ–‡ä»¶ï¼Œå¦å­˜ä¸ºè¾ƒæ–°çš„æ ¼å¼\n"
            error_msg += "3. ç¡®è®¤æ–‡ä»¶æœªæŸåä¸”åŒ…å«æ–‡æœ¬å†…å®¹ï¼ˆè€Œéçº¯å›¾ç‰‡ï¼‰\n"
            error_msg += "4. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å¯†ç ä¿æŠ¤æˆ–è®¿é—®é™åˆ¶\n"
        elif file_type == 'pdf':
            error_msg += "1. ç¡®è®¤PDFåŒ…å«å¯é€‰æ‹©çš„æ–‡æœ¬ï¼ˆè€Œéæ‰«æå›¾ç‰‡ï¼‰\n"
            error_msg += "2. å°†PDFè½¬æ¢ä¸ºWordæ–‡æ¡£æ ¼å¼\n"
            error_msg += "3. ä½¿ç”¨OCRå·¥å…·è½¬æ¢æ‰«æç‰ˆPDF\n"
        else:
            error_msg += "1. æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œç¼–ç æ˜¯å¦æ­£ç¡®\n"
            error_msg += "2. å°è¯•ä½¿ç”¨å…¶ä»–è½¯ä»¶é‡æ–°ä¿å­˜æ–‡ä»¶\n"
            error_msg += "3. ç¡®è®¤æ–‡ä»¶åŒ…å«å¯è¯»çš„æ–‡æœ¬å†…å®¹\n"
            
        error_msg += "\nå¦‚é—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒå¹¶æä¾›æ–‡ä»¶æ ·æœ¬ã€‚"
        
        return error_msg
    
    def _classify_error_type(self, exception: Exception, error_msg: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_msg_lower = error_msg.lower()
        
        # æ–‡æ¡£å¤„ç†ç›¸å…³é”™è¯¯
        if "encoding" in error_msg_lower or "decode" in error_msg_lower or "æ— æ³•ä»æ–‡ä»¶ä¸­æå–å¯è¯»æ–‡æœ¬" in error_msg:
            return "encoding_error"
        elif "file size" in error_msg_lower or "exceeds limit" in error_msg_lower:
            return "file_size_error"
        elif "unsupported file format" in error_msg_lower or "format" in error_msg_lower:
            return "format_error"
        elif "no text content" in error_msg_lower or "empty" in error_msg_lower:
            return "empty_content_error"
        elif "corrupted" in error_msg_lower or "ä¹±ç " in error_msg:
            return "corruption_error"
        
        # AIæœåŠ¡ç›¸å…³é”™è¯¯
        elif "api" in error_msg_lower and ("timeout" in error_msg_lower or "connection" in error_msg_lower):
            return "ai_service_timeout"
        elif "api" in error_msg_lower and ("rate limit" in error_msg_lower or "quota" in error_msg_lower):
            return "ai_service_quota"
        elif "api" in error_msg_lower and "key" in error_msg_lower:
            return "ai_service_auth"
        elif "ai" in error_msg_lower or "openai" in error_msg_lower or "zhipu" in error_msg_lower:
            return "ai_service_error"
        
        # ç³»ç»Ÿèµ„æºé”™è¯¯
        elif "memory" in error_msg_lower or "ram" in error_msg_lower:
            return "memory_error"
        elif "disk" in error_msg_lower or "space" in error_msg_lower:
            return "disk_error"
        elif "timeout" in error_msg_lower:
            return "timeout_error"
        
        # æƒé™é”™è¯¯
        elif "permission" in error_msg_lower or "access" in error_msg_lower:
            return "permission_error"
        
        # æœªçŸ¥é”™è¯¯
        else:
            return "unknown_error"
    
    def _generate_detailed_error_message(self, exception: Exception, error_type: str, 
                                       filename: str, doc_info: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯"""
        base_error = str(exception)
        
        error_messages = {
            "encoding_error": {
                "title": "æ–‡æ¡£ç¼–ç é—®é¢˜",
                "message": f"æ— æ³•æ­£ç¡®è¯»å–æ–‡æ¡£ '{filename}' çš„æ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜",
                "details": [
                    "æ£€æµ‹åˆ°æ–‡æ¡£ç¼–ç æ ¼å¼ä¸å…¼å®¹æˆ–æ–‡ä»¶æŸå",
                    "æ–‡æ¡£å¯èƒ½ä½¿ç”¨äº†ä¸æ”¯æŒçš„å­—ç¬¦ç¼–ç ",
                    "æ–‡ä»¶å†…å®¹å¯èƒ½åŒ…å«ä¹±ç æˆ–ç‰¹æ®Šå­—ç¬¦"
                ],
                "suggestions": [
                    "å°è¯•å°†æ–‡æ¡£è½¬æ¢ä¸ºUTF-8ç¼–ç æ ¼å¼",
                    "å°†.docæ–‡ä»¶è½¬æ¢ä¸º.docxæ ¼å¼",
                    "ä½¿ç”¨Microsoft Wordé‡æ–°ä¿å­˜æ–‡æ¡£",
                    "ç¡®è®¤æ–‡æ¡£å®Œæ•´ä¸”æœªæŸå"
                ]
            },
            "format_error": {
                "title": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼",
                "message": f"æ–‡ä»¶ '{filename}' çš„æ ¼å¼ä¸å—æ”¯æŒ",
                "details": [
                    f"æ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»å‹: {doc_info.get('mimetype', 'æœªçŸ¥')}",
                    "ç³»ç»Ÿåªæ”¯æŒå¸¸è§çš„åŠå…¬æ–‡æ¡£æ ¼å¼",
                    "è¯·ç¡®è®¤æ–‡ä»¶æ‰©å±•åä¸å®é™…å†…å®¹åŒ¹é…"
                ],
                "suggestions": [
                    "æ”¯æŒæ ¼å¼: PDF, DOCX, DOC, XLSX, XLS, PPTX, TXT",
                    "å°†æ–‡ä»¶è½¬æ¢ä¸ºæ”¯æŒçš„æ ¼å¼",
                    "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸‹è½½",
                    "ç¡®è®¤æ–‡ä»¶æœªè¢«åŠ å¯†æˆ–å¯†ç ä¿æŠ¤"
                ]
            },
            "file_size_error": {
                "title": "æ–‡ä»¶å¤§å°è¶…é™",
                "message": f"æ–‡ä»¶ '{filename}' å¤§å°è¶…å‡ºç³»ç»Ÿé™åˆ¶",
                "details": [
                    f"å½“å‰æ–‡ä»¶å¤§å°: {doc_info.get('size', 0) / 1024 / 1024:.1f}MB",
                    "ç³»ç»Ÿé™åˆ¶: 10MB",
                    "å¤§æ–‡ä»¶å¯èƒ½å½±å“å¤„ç†æ€§èƒ½å’Œç¨³å®šæ€§"
                ],
                "suggestions": [
                    "å‹ç¼©æ–‡æ¡£æˆ–å‡å°‘å†…å®¹",
                    "åˆ†å‰²å¤§æ–‡æ¡£ä¸ºå¤šä¸ªå°æ–‡æ¡£",
                    "ç§»é™¤æ–‡æ¡£ä¸­çš„å¤§å›¾ç‰‡æˆ–åª’ä½“æ–‡ä»¶",
                    "ä½¿ç”¨PDFæ ¼å¼å¯ä»¥æœ‰æ•ˆå‡å°æ–‡ä»¶å¤§å°"
                ]
            },
            "empty_content_error": {
                "title": "æ–‡æ¡£å†…å®¹ä¸ºç©º",
                "message": f"æ–‡æ¡£ '{filename}' ä¸­æ²¡æœ‰å‘ç°å¯è¯»çš„æ–‡æœ¬å†…å®¹",
                "details": [
                    "æ–‡æ¡£å¯èƒ½åªåŒ…å«å›¾ç‰‡æˆ–å›¾è¡¨",
                    "æ–‡æ¡£å¯èƒ½æ˜¯æ‰«æç‰ˆæœ¬",
                    "æ–‡æ¡£å¯èƒ½å­˜åœ¨æ ¼å¼é—®é¢˜"
                ],
                "suggestions": [
                    "ç¡®è®¤æ–‡æ¡£åŒ…å«æ–‡æœ¬å†…å®¹è€Œéçº¯å›¾ç‰‡",
                    "å¯¹äºæ‰«ææ–‡æ¡£ï¼Œè¯·ä½¿ç”¨OCRè½¯ä»¶è½¬æ¢",
                    "æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ­£ç¡®æ‰“å¼€",
                    "å°è¯•é‡æ–°åˆ›å»ºæˆ–å¯¼å‡ºæ–‡æ¡£"
                ]
            },
            "corruption_error": {
                "title": "æ–‡æ¡£å†…å®¹æŸå",
                "message": f"æ–‡æ¡£ '{filename}' çš„å†…å®¹å‡ºç°æŸåæˆ–ä¸¥é‡ä¹±ç ",
                "details": [
                    "æ£€æµ‹åˆ°å¤§é‡ä¸å¯è¯»å­—ç¬¦",
                    "æ–‡æ¡£å¯èƒ½åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­æŸå",
                    "æ–‡æ¡£ç¼–ç ä¸å®é™…å†…å®¹ä¸åŒ¹é…"
                ],
                "suggestions": [
                    "é‡æ–°ä¸‹è½½æˆ–è·å–åŸå§‹æ–‡æ¡£",
                    "ä½¿ç”¨åŸå§‹è½¯ä»¶é‡æ–°ä¿å­˜æ–‡æ¡£",
                    "æ£€æŸ¥æ–‡æ¡£çš„å®Œæ•´æ€§",
                    "è”ç³»æ–‡æ¡£æä¾›æ–¹ç¡®è®¤æ–‡æ¡£çŠ¶æ€"
                ]
            },
            "ai_service_timeout": {
                "title": "AIæœåŠ¡è¶…æ—¶",
                "message": "AIåˆ†ææœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                "details": [
                    "AIæœåŠ¡å½“å‰è´Ÿè½½è¾ƒé«˜",
                    "ç½‘ç»œè¿æ¥å¯èƒ½ä¸ç¨³å®š",
                    "æ–‡æ¡£å†…å®¹å¯èƒ½è¿‡äºå¤æ‚"
                ],
                "suggestions": [
                    "ç¨åé‡æ–°å°è¯•åˆ†æ",
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€",
                    "å°è¯•åˆ†æè¾ƒç®€å•çš„æ–‡æ¡£",
                    "è”ç³»ç³»ç»Ÿç®¡ç†å‘˜æ£€æŸ¥æœåŠ¡çŠ¶æ€"
                ]
            },
            "ai_service_quota": {
                "title": "AIæœåŠ¡é…é¢ä¸è¶³",
                "message": "AIæœåŠ¡ä½¿ç”¨é‡å·²è¾¾åˆ°é™åˆ¶",
                "details": [
                    "å½“å‰æ—¶æ®µçš„APIè°ƒç”¨æ¬¡æ•°å·²ç”¨å®Œ",
                    "å¯èƒ½éœ€è¦ç­‰å¾…é…é¢é‡ç½®",
                    "æˆ–éœ€è¦å‡çº§æœåŠ¡è®¡åˆ’"
                ],
                "suggestions": [
                    "ç­‰å¾…é…é¢é‡ç½®ï¼ˆé€šå¸¸ä¸º24å°æ—¶ï¼‰",
                    "è”ç³»ç®¡ç†å‘˜å‡çº§æœåŠ¡è®¡åˆ’",
                    "æš‚æ—¶ä½¿ç”¨å…¶ä»–åˆ†æå·¥å…·",
                    "å‡å°‘åŒæ—¶è¿›è¡Œçš„åˆ†æä»»åŠ¡"
                ]
            },
            "ai_service_auth": {
                "title": "AIæœåŠ¡è®¤è¯å¤±è´¥",
                "message": "AIæœåŠ¡è®¤è¯å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ",
                "details": [
                    "APIå¯†é’¥å¯èƒ½å·²è¿‡æœŸ",
                    "æœåŠ¡é…ç½®å¯èƒ½ä¸æ­£ç¡®",
                    "è®¤è¯ä¿¡æ¯å¯èƒ½è¢«æ›´æ”¹"
                ],
                "suggestions": [
                    "è”ç³»ç³»ç»Ÿç®¡ç†å‘˜æ£€æŸ¥APIé…ç½®",
                    "ç¡®è®¤æœåŠ¡è®¢é˜…çŠ¶æ€",
                    "æ£€æŸ¥ç½‘ç»œé˜²ç«å¢™è®¾ç½®",
                    "å°è¯•é‡æ–°å¯åŠ¨æœåŠ¡"
                ]
            },
            "ai_service_error": {
                "title": "AIæœåŠ¡é”™è¯¯",
                "message": "AIåˆ†ææœåŠ¡å‡ºç°é”™è¯¯ï¼Œæ— æ³•å®Œæˆåˆ†æ",
                "details": [
                    "AIæœåŠ¡å†…éƒ¨å‡ºç°é—®é¢˜",
                    "å¯èƒ½æ˜¯ä¸´æ—¶æ€§æ•…éšœ",
                    base_error if base_error else "æœªçŸ¥AIæœåŠ¡é”™è¯¯"
                ],
                "suggestions": [
                    "ç¨åé‡æ–°å°è¯•",
                    "æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦ç¬¦åˆè¦æ±‚",
                    "å°è¯•åˆ†æå…¶ä»–æ–‡æ¡£",
                    "è”ç³»æŠ€æœ¯æ”¯æŒ"
                ]
            },
            "memory_error": {
                "title": "å†…å­˜ä¸è¶³",
                "message": "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç†æ­¤æ–‡æ¡£",
                "details": [
                    "æ–‡æ¡£è¿‡å¤§æˆ–è¿‡äºå¤æ‚",
                    "ç³»ç»Ÿå½“å‰è´Ÿè½½è¾ƒé«˜",
                    "å†…å­˜èµ„æºä¸´æ—¶ä¸è¶³"
                ],
                "suggestions": [
                    "ç¨åé‡è¯•",
                    "å°è¯•åˆ†æè¾ƒå°çš„æ–‡æ¡£",
                    "åˆ†æ‰¹å¤„ç†å¤§å‹æ–‡æ¡£",
                    "è”ç³»ç®¡ç†å‘˜ä¼˜åŒ–ç³»ç»Ÿèµ„æº"
                ]
            },
            "timeout_error": {
                "title": "å¤„ç†è¶…æ—¶",
                "message": "æ–‡æ¡£å¤„ç†è¶…æ—¶ï¼Œåˆ†ææœªèƒ½å®Œæˆ",
                "details": [
                    "æ–‡æ¡£å¤„ç†æ—¶é—´è¶…è¿‡ç³»ç»Ÿé™åˆ¶",
                    "æ–‡æ¡£å¯èƒ½è¿‡äºå¤æ‚",
                    "ç³»ç»Ÿå½“å‰å¤„ç†è´Ÿè½½è¾ƒé«˜"
                ],
                "suggestions": [
                    "ç¨åé‡è¯•",
                    "ç®€åŒ–æ–‡æ¡£å†…å®¹",
                    "åˆ†æ®µå¤„ç†å¤æ‚æ–‡æ¡£",
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§"
                ]
            },
            "unknown_error": {
                "title": "æœªçŸ¥é”™è¯¯",
                "message": f"å¤„ç†æ–‡æ¡£ '{filename}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯",
                "details": [
                    "ç³»ç»Ÿé‡åˆ°äº†é¢„æœŸä¹‹å¤–çš„é—®é¢˜",
                    base_error if base_error else "é”™è¯¯è¯¦æƒ…ä¸æ˜",
                    "å¯èƒ½æ˜¯ç³»ç»Ÿæˆ–æ–‡æ¡£çš„ç‰¹æ®Šæƒ…å†µ"
                ],
                "suggestions": [
                    "é‡æ–°å°è¯•ä¸Šä¼ å’Œåˆ†æ",
                    "æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ­£å¸¸",
                    "å°è¯•å…¶ä»–æ ¼å¼çš„æ–‡æ¡£",
                    "è”ç³»æŠ€æœ¯æ”¯æŒå¹¶æä¾›æ–‡æ¡£æ ·æœ¬"
                ]
            }
        }
        
        error_info = error_messages.get(error_type, error_messages["unknown_error"])
        
        return {
            "message": error_info["message"],
            "details": error_info["details"],
            "suggestions": error_info["suggestions"],
            "title": error_info["title"]
        }
    
    def get_supported_formats(self) -> Dict[str, Any]:
        """è·å–æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ä¿¡æ¯"""
        doc_formats = self.document_processor.get_supported_formats()
        ai_available = self.ai_client.is_available()
        
        return {
            'document_formats': doc_formats,
            'ai_available': ai_available,
            'features': {
                'pdf_extraction': doc_formats['availability']['pdf'],
                'docx_extraction': doc_formats['availability']['docx'],
                'ocr_extraction': doc_formats['availability']['ocr'],
                'ai_analysis': ai_available
            }
        }
    
    def validate_analysis_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯åˆ†æç»“æœçš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
        
        Args:
            result: AIåˆ†æç»“æœ
            
        Returns:
            Dict: éªŒè¯ç»“æœå’Œå»ºè®®
        """
        validation = {
            'valid': True,
            'warnings': [],
            'suggestions': [],
            'completeness_score': 0
        }
        
        extracted_data = result.get('extracted_data', {})
        confidence_scores = result.get('confidence_scores', {})
        
        # æ£€æŸ¥åŸºç¡€ä¿¡æ¯å®Œæ•´æ€§
        basic_info = extracted_data.get('basic_info', {})
        required_fields = ['name', 'code', 'category']
        missing_fields = [field for field in required_fields if not basic_info.get(field)]
        
        if missing_fields:
            validation['warnings'].append(f"Missing basic info: {', '.join(missing_fields)}")
            validation['suggestions'].append("Please provide the missing basic product information")
        
        # æ£€æŸ¥ç½®ä¿¡åº¦
        overall_confidence = confidence_scores.get('overall', 0)
        if overall_confidence < 0.5:
            validation['warnings'].append("Low overall confidence in analysis results")
            validation['suggestions'].append("Consider reviewing and manually verifying the extracted information")
        
        # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        completeness_factors = [
            1 if basic_info.get('name') else 0,
            1 if basic_info.get('code') else 0,
            1 if basic_info.get('category') else 0,
            1 if basic_info.get('description') else 0,
            1 if extracted_data.get('specifications') else 0,
            1 if extracted_data.get('features') else 0
        ]
        
        validation['completeness_score'] = sum(completeness_factors) / len(completeness_factors)
        
        # åŸºäºå®Œæ•´æ€§ç»™å‡ºå»ºè®®
        if validation['completeness_score'] < 0.6:
            validation['suggestions'].append("Consider providing additional product documentation for better analysis")
        
        return validation
    
    def generate_analysis_summary(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æç»“æœæ‘˜è¦"""
        if not result.get('success'):
            return f"Analysis failed: {result.get('error', 'Unknown error')}"
        
        extracted_data = result.get('extracted_data', {})
        basic_info = extracted_data.get('basic_info', {})
        confidence = result.get('confidence_scores', {}).get('overall', 0)
        
        summary_parts = []
        
        # äº§å“åç§°
        if basic_info.get('name'):
            summary_parts.append(f"Product: {basic_info['name']}")
        
        # å‹å·
        if basic_info.get('code'):
            summary_parts.append(f"Model: {basic_info['code']}")
        
        # åˆ†ç±»
        if basic_info.get('category'):
            summary_parts.append(f"Category: {basic_info['category']}")
        
        # è§„æ ¼æ•°é‡
        specs_count = len(extracted_data.get('specifications', {}))
        if specs_count > 0:
            summary_parts.append(f"Specifications: {specs_count} items")
        
        # ç‰¹æ€§æ•°é‡
        features_count = len(extracted_data.get('features', []))
        if features_count > 0:
            summary_parts.append(f"Features: {features_count} items")
        
        # ç½®ä¿¡åº¦
        summary_parts.append(f"Confidence: {confidence:.1%}")
        
        return " | ".join(summary_parts) if summary_parts else "Analysis completed with limited information"
    
    def learn_from_user_modifications(self, analysis_record_id: int, 
                                    original_data: Dict[str, Any], 
                                    final_data: Dict[str, Any],
                                    user_modifications: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»ç”¨æˆ·ä¿®æ­£ä¸­å­¦ä¹ 
        
        Args:
            analysis_record_id: åˆ†æè®°å½•ID
            original_data: åŸå§‹AIæå–æ•°æ®
            final_data: æœ€ç»ˆç¡®è®¤æ•°æ®
            user_modifications: ç”¨æˆ·ä¿®æ­£è®°å½•
            
        Returns:
            Dict: å­¦ä¹ ç»“æœ
        """
        try:
            learning_result = self.learning_engine.learn_from_modifications(
                analysis_record_id=analysis_record_id,
                original_data=original_data,
                final_data=final_data,
                user_modifications=user_modifications
            )
            
            logger.info(f"Learning completed for record {analysis_record_id}: "
                       f"{learning_result.get('patterns_identified', 0)} patterns")
            
            return learning_result
            
        except Exception as e:
            logger.error(f"Learning from modifications failed: {str(e)}")
            return {
                'patterns_identified': 0,
                'error': str(e)
            }
    
    def get_personalized_analysis_hints(self, user_id: int, document_type: str, 
                                      extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–ä¸ªæ€§åŒ–åˆ†ææç¤º
        
        Args:
            user_id: ç”¨æˆ·ID
            document_type: æ–‡æ¡£ç±»å‹
            extracted_data: æå–çš„æ•°æ®
            
        Returns:
            Dict: ä¸ªæ€§åŒ–æç¤º
        """
        try:
            hints = self.learning_engine.get_personalized_hints(
                user_id=user_id,
                document_type=document_type,
                extracted_data=extracted_data
            )
            
            return hints
            
        except Exception as e:
            logger.error(f"Failed to get personalized hints: {str(e)}")
            return {
                'personalized_hints': [],
                'predicted_modifications': {},
                'error': str(e)
            }
    
    def optimize_analysis_for_document_type(self, document_type: str, 
                                          category: str = None) -> Dict[str, Any]:
        """
        ä¸ºç‰¹å®šæ–‡æ¡£ç±»å‹ä¼˜åŒ–åˆ†æ
        
        Args:
            document_type: æ–‡æ¡£ç±»å‹
            category: äº§å“åˆ†ç±»
            
        Returns:
            Dict: ä¼˜åŒ–é…ç½®
        """
        try:
            optimization = self.learning_engine.optimize_extraction_prompt(
                document_type=document_type,
                category=category
            )
            
            return optimization
            
        except Exception as e:
            logger.error(f"Failed to optimize analysis: {str(e)}")
            return {
                'prompt_enhancements': {},
                'common_errors': [],
                'error': str(e)
            }
    
    def get_learning_statistics(self, days: int = 30) -> Dict[str, Any]:
        """
        è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            days: ç»Ÿè®¡å¤©æ•°
            
        Returns:
            Dict: å­¦ä¹ ç»Ÿè®¡
        """
        try:
            stats = self.learning_engine.get_learning_statistics(days=days)
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get learning statistics: {str(e)}")
            return {
                'total_modifications': 0,
                'avg_modification_rate': 0.0,
                'error': str(e)
            }